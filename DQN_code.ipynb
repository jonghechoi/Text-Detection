{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cion.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "13t39025Cm7l9xX-6oAuMO3EMgMx5Y6LL",
      "authorship_tag": "ABX9TyNMvTNOR9z1ICH/qWlbe4o/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jonghechoi/jonghe/blob/master/DQN_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dP0J0cP2iye-",
        "colab_type": "code",
        "outputId": "12f9a82f-328f-43ab-ce14-70517eacc26b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "pip install pygame"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pygame\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/24/ede6428359f913ed9cd1643dd5533aefeb5a2699cc95bea089de50ead586/pygame-1.9.6-cp36-cp36m-manylinux1_x86_64.whl (11.4MB)\n",
            "\u001b[K     |████████████████████████████████| 11.4MB 4.7MB/s \n",
            "\u001b[?25hInstalling collected packages: pygame\n",
            "Successfully installed pygame-1.9.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLYRy1xVl-hI",
        "colab_type": "code",
        "outputId": "e67c3fd7-289c-40c8-893d-1c016bdab3ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import os\n",
        "import pickle\n",
        "import pygame \n",
        "import random \n",
        "import numpy as np\n",
        "from collections import deque\n",
        "from skimage.color import rgb2gray\n",
        "from skimage.transform import resize\n",
        "from matplotlib import pyplot as plt\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers import Dense, Flatten\n",
        "from keras.optimizers import RMSprop\n",
        "\n",
        "# GCP서버의 os 우분투에는 SDL이 제대로 설정되어 있지 않아 pygame.display를 불러오지 못한다(error: Not available Video device )\n",
        "# 따라서 아래와 같은 처리를 해줘야 한다.\n",
        "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n",
        "\n",
        "FIGHTER_MOVE = [-10,10,0]\n",
        "\n",
        "EPISODES = 30000\n",
        "\n",
        "class Agent:\n",
        "    \n",
        "    def __init__(self,gamepad,fighter, action_size ):\n",
        "        self.gamepad = gamepad\n",
        "        self.fighter = fighter\n",
        "        self.fires = []\n",
        "        self.action_size = action_size\n",
        "        self.state_size = (100, 80, 4)\n",
        "        \n",
        "        # DQN 하이퍼파라미터\n",
        "        self.epsilon = 1.\n",
        "        self.epsilon_start, self.epsilon_end = 1.0, 0.1\n",
        "        self.exploration_steps = 10000.\n",
        "        self.epsilon_decay_step = (self.epsilon_start - self.epsilon_end) / self.exploration_steps\n",
        "        self.batch_size = 500\n",
        "        self.train_start = 8000\n",
        "        self.update_target_rate = 10000\n",
        "        self.discount_factor = 0.99        \n",
        "        \n",
        "        # 리플레이 메모리, 최대 크기 10000\n",
        "        self.memory = deque(maxlen=10000)\n",
        "        \n",
        "        # 모델과 타겟모델을 생성하고 타겟모델 초기화\n",
        "        self.model = self.build_model()\n",
        "        self.target_model = self.build_model()\n",
        "        self.update_target_model()\n",
        "        self.optimizer = self.optimizer()\n",
        "        \n",
        "        self.avg_q_max, self.avg_loss = 0, 0\n",
        "        \n",
        "    # reset, addfire 함수는 pygame이 작동되기 위한 기본코드이다.\n",
        "    def reset(self):\n",
        "        for fire in self.fires:\n",
        "            fire.reset()\n",
        "        self.fighter.reset()    \n",
        "    \n",
        "        \n",
        "    def addfire(self, fire):\n",
        "        self.fires.append(fire)\n",
        "        \n",
        "    def optimizer(self):\n",
        "        a = K.placeholder(shape=(None,), dtype='int32')\n",
        "        y = K.placeholder(shape=(None,), dtype='float32')\n",
        "\n",
        "        prediction = self.model.output\n",
        "\n",
        "        # action(0 or 1 or 2)이 원-핫인코딩 되고\n",
        "        # 원-핫인코딩 [x,y,z]의 값이 model의 예측값과 곱해진다.\n",
        "        # 이것은 다시 axis=1로 sum이 되고 결국은 행해진 action의 Q값만이 출력된다.\n",
        "        a_one_hot = K.one_hot(a, self.action_size)\n",
        "        q_value = K.sum(prediction * a_one_hot, axis=1)  \n",
        "        error = K.abs(y - q_value)\n",
        "        quadratic_part = K.clip(error, 0.0, 1.0)\n",
        "        linear_part = error - quadratic_part\n",
        "        loss = K.mean(0.5 * K.square(quadratic_part) + linear_part)\n",
        "\n",
        "        optimizer = RMSprop(lr=0.00025, epsilon=0.01)\n",
        "        updates = optimizer.get_updates(self.model.trainable_weights, [], loss)\n",
        "        train = K.function([self.model.input, a, y], [loss], updates=updates)        \n",
        "\n",
        "        return train\n",
        "\n",
        "    def build_model(self):\n",
        "        model = Sequential()\n",
        "        model.add(Conv2D(32, (8, 8), strides=(4, 4), activation='relu', input_shape=self.state_size))\n",
        "        model.add(Conv2D(64, (4, 4), strides=(2, 2), activation='relu'))\n",
        "        #model.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu'))\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(256, activation='relu'))\n",
        "        model.add(Dense(256, activation='relu'))\n",
        "        model.add(Dense(64, activation='relu'))\n",
        "        model.add(Dense(self.action_size))\n",
        "        model.summary()\n",
        "\n",
        "        return model\n",
        "\n",
        "    # 타겟 모델을 모델의 가중치로 업데이트\n",
        "    def update_target_model(self):\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "\n",
        "    def train_model(self):\n",
        "        if self.epsilon > self.epsilon_end:                              # epsilon의 초기값 1에서 episilon_decay_step(0.00000009)만큼 게속 줄어들고\n",
        "            self.epsilon -= self.epsilon_decay_step                      # epsilon이 0.1보다 작아지면\n",
        "                                                                         \n",
        "        mini_batch = random.sample(self.memory, self.batch_size)\n",
        "\n",
        "        history = np.zeros((self.batch_size, self.state_size[0], self.state_size[1], self.state_size[2]))\n",
        "        next_history = np.zeros((self.batch_size, self.state_size[0], self.state_size[1], self.state_size[2]))\n",
        "        target = np.zeros((self.batch_size,))\n",
        "        action, reward = [], []\n",
        "\n",
        "        for i in range(self.batch_size):\n",
        "            history[i] = np.float32(mini_batch[i][0] / 255.)\n",
        "            next_history[i] = np.float32(mini_batch[i][3] / 255.)\n",
        "            action.append(mini_batch[i][1])\n",
        "            reward.append(mini_batch[i][2])\n",
        "\n",
        "\n",
        "        target_value = self.target_model.predict(next_history)\n",
        "\n",
        "        for i in range(self.batch_size):\n",
        "            if reward[i] < 0:\n",
        "                target[i] = reward[i]\n",
        "            else:\n",
        "                target[i] = reward[i] + self.discount_factor * np.amax(target_value[i])\n",
        "\n",
        "        loss = self.optimizer([history, action, target])\n",
        "        self.avg_loss += loss[0]\n",
        "\n",
        "\n",
        "    \n",
        "    # 입실론 탐욕 정책으로 행동 선택(exploit & expor;lation)\n",
        "    def get_action(self, history):\n",
        "        history = np.float32(history / 255.0)\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            direction = random.randrange(self.action_size)\n",
        "            x = FIGHTER_MOVE[direction]\n",
        "            self.fighter.setPos(x)\n",
        "            self.fighter.move()\n",
        "            \n",
        "            return direction\n",
        "        \n",
        "        else:\n",
        "            q_value = self.model. predict(history)\n",
        "            direction = np.argmax(q_value[0])\n",
        "            x = FIGHTER_MOVE[direction]\n",
        "            self.fighter.setPos(x)\n",
        "            self.fighter.move() \n",
        "            \n",
        "            return direction\n",
        "    \n",
        "    # 샘플 <s, a, r, s'>을 리플레이 메모리에 저장\n",
        "    def append_sample(self, history, action, reward, next_history):\n",
        "        self.memory.append((history, action, reward, next_history))\n",
        "\n",
        "    \n",
        "\n",
        "class Fighter:\n",
        "    def __init__(self,background,gamepad):\n",
        "        global pad_width, pad_height \n",
        "        \n",
        "        self.background = background\n",
        "        self.gamepad = gamepad\n",
        "        self.fighter = pygame.image.load('/content/drive/My Drive/test_colab/fighter.png')\n",
        "        self.pad_width = 600\n",
        "        self.pad_height = 700\n",
        "        \n",
        "        # 전투기 초기 위치(x,y)\n",
        "        self.x = pad_width * 0.45\n",
        "        self.y = pad_height * 0.88\n",
        "        self.gamepad.blit(self.fighter,(self.x,self.y))\n",
        "        self.fighter_width = 50\n",
        "        self.fighter_height = 70\n",
        "        self.x_change = 0\n",
        "        \n",
        "    def reset(self):\n",
        "        self.x = pad_width * 0.45\n",
        "        self.y = pad_height * 0.88\n",
        "        self.gamepad.blit(self.fighter,(self.x,self.y))\n",
        "        self.fighter_width = 50\n",
        "        self.fighter_height = 70\n",
        "        self.x_change = 0\n",
        "\n",
        "        \n",
        "    def move(self):\n",
        "        for event in pygame.event.get():\n",
        "            if event.type == pygame.QUIT: # 마우스로 창을 닫는 이벤트\n",
        "                pygame.quit()\n",
        "            if event.type == pygame.KEYDOWN:\n",
        "                if event.key == pygame.K_LEFT:\n",
        "                    self.x_change -= 8\n",
        "                elif  event.key == pygame.K_RIGHT:\n",
        "                    self.x_change += 8\n",
        "            if event.type == pygame.KEYUP:\n",
        "                if event.key == pygame.K_LEFT or event.key == pygame.K_RIGHT:\n",
        "                    self.x_change = 0\n",
        "        \n",
        "        # 전투기 위치를 재조정\n",
        "        self.x += self.x_change      # 현재 전투기 x좌표\n",
        "                       \n",
        "        if self.x < 0:\n",
        "            self.x = 0\n",
        "        elif self.x > self.pad_width - self.fighter_width:\n",
        "            self.x = self.pad_width - self.fighter_width\n",
        "\n",
        "        self.gamepad.blit(self.fighter,(self.x,self.y))\n",
        "        \n",
        "    def setPos(self, x):\n",
        "        self.x_change = x\n",
        "        \n",
        "   \n",
        "                \n",
        "\n",
        "class Fire:\n",
        "    def __init__(self,gamepad, fighter, speed):\n",
        "        global fire_width, fire_height\n",
        "        \n",
        "        self.gamepad = gamepad\n",
        "        self.fighter = fighter\n",
        "        self.fire_width = 50\n",
        "        self.fire_height = 70\n",
        "        self.speed = speed\n",
        "        self.fire = pygame.image.load('/content/drive/My Drive/test_colab/fire2.png')\n",
        "        self.fire_x = random.randrange(0, pad_width-self.fire_width)\n",
        "        self.fire_y = -5 \n",
        "\n",
        "\n",
        "    def move(self):\n",
        "        if self.fire_y >= pad_height:\n",
        "            self.fire = pygame.image.load('/content/drive/My Drive/test_colab/fire2.png')\n",
        "            self.fire_x = random.randrange(0, pad_width-self.fire_width)\n",
        "            self.fire_y = -5\n",
        "            \n",
        "        self.fire_y += self.speed\n",
        "                \n",
        "        self.gamepad.blit(self.fire,(self.fire_x,self.fire_y))\n",
        "\n",
        "\n",
        "    def reset(self):\n",
        "        self.fire = pygame.image.load('/content/drive/My Drive/test_colab/fire2.png')\n",
        "        self.fire_x = random.randrange(0, pad_width-self.fire_width)\n",
        "        self.fire_y = -5 \n",
        "        pos = [self.fire_x, self.fire_y]\n",
        "        if pos[1] >= pad_height:\n",
        "            self.fire = pygame.image.load('/content/drive/My Drive/test_colab/fire2.png')\n",
        "            pos[0] = random.randrange(0, pad_width-self.fire_width)\n",
        "            pos[1] = -5\n",
        "            \n",
        "        pos[1] += self.speed\n",
        "                \n",
        "        self.gamepad.blit(self.fire,(pos[0],pos[1]))\n",
        "\n",
        "        \n",
        "        \n",
        "    def hit(self,pos):\n",
        "        fpos = [self.fighter.x, self.fighter.y]\n",
        "        if pos[1] + self.fire_height > fpos[1] or pos[1] > fpos[1] + self.fighter.fighter_height :\n",
        "            if (pos[0] > fpos[0] and pos[0] < fpos[0] + self.fighter.fighter_width) or \\\n",
        "            (pos[0] + self.fire_width > fpos[0] and pos[0] + self.fire_width < fpos[0] + self.fighter.fighter_width):\n",
        "                return True\n",
        "        else:\n",
        "            return False\n",
        "            \n",
        "    \n",
        "    def is_hit(self):\n",
        "        pos = [self.fire_x, self.fire_y]\n",
        "        return self.hit(pos)\n",
        "        \n",
        "    \n",
        "    def is_bottom_hit(self):\n",
        "        pos = [self.fire_x, self.fire_y]\n",
        "        if pos[1] >= pad_height:\n",
        "            return True\n",
        "        return False\n",
        "    \n",
        "\n",
        "    def setPos(self,x,y):\n",
        "        self.fire_x = x\n",
        "        self.fire_y = y\n",
        "        \n",
        "        \n",
        "        \n",
        "\n",
        "class Background:\n",
        "    def __init__(self, gamepad):\n",
        "        global pad_width, pad_height\n",
        "        \n",
        "        self.gamepad = gamepad\n",
        "        self.gamepad.fill((1,1,1))\n",
        "        self.space1 = pygame.image.load('/content/drive/My Drive/test_colab/back.png')\n",
        "        self.space2 = self.space1.copy()\n",
        "\n",
        "        self.background_height = -700\n",
        "        self.background_y = 0\n",
        "        self.background2_y = self.background_height\n",
        "\n",
        "    def move(self):        \n",
        "        self.background_y += 2\n",
        "        self.background2_y += 2\n",
        "        \n",
        "        if self.background_y == -self.background_height:\n",
        "            self.background_y = self.background_height\n",
        "            \n",
        "        if self.background2_y == -self.background_height:\n",
        "            self.background2_y = self.background_height\n",
        "        \n",
        "        self.gamepad.blit(self.space1,(0,self.background_y))\n",
        "        self.gamepad.blit(self.space2,(0,self.background2_y))\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "# 전처리 코드\n",
        "def pre_processing(gamepad):\n",
        "    a = pygame.surfarray.array3d(gamepad)\n",
        "    aa = a.transpose(1, 0, 2)\n",
        "    processed_img = np.uint8(resize(rgb2gray(aa), (100, 80), mode='constant') * 255)\n",
        "\n",
        "    #plt.imshow( processed_img, cmap='gray')\n",
        "    #plt.show()\n",
        "\n",
        "    return processed_img\n",
        "\n",
        "\n",
        "# 실행 코드\n",
        "if __name__ == \"__main__\":\n",
        "    pygame.init()\n",
        "    pygame.display.set_caption('흥만티 Game') \n",
        "    pad_width = 600  \n",
        "    pad_height = 700\n",
        "    gamepad = pygame.display.set_mode((pad_width, pad_height))\n",
        "    background = Background(gamepad)\n",
        "    fighter = Fighter(background,gamepad)\n",
        "    clock = pygame.time.Clock() #초당 프레임수를 설정할 수 있는 Clock객체 생성\n",
        "    \n",
        "    fire1 = Fire(gamepad, fighter, speed = 12)\n",
        "    fire2 = Fire( gamepad, fighter, speed = 11)\n",
        "    fire3 = Fire( gamepad, fighter, speed = 15)\n",
        "\n",
        "    agent = Agent(gamepad,fighter,action_size=3)\n",
        "    \n",
        "    agent.addfire(fire1)\n",
        "    agent.addfire(fire2)\n",
        "    agent.addfire(fire3)\n",
        "\n",
        "    num_of_evasions, episodes, global_step = [], [], 0\n",
        "    average_loss_list = []\n",
        "    \n",
        "    for e in range(EPISODES):\n",
        "        done = False\n",
        "        \n",
        "        step, reward, num_of_evasion, frame_skip = 0, 0, 0, 0 # score와 start_life는 시온이한테는 필요 없다. 점수나 불에 맞은 횟수를 측정할 필요는 없다.\n",
        "    \n",
        "        while not done:\n",
        "            clock.tick(1000)\n",
        "            global_step += 1\n",
        "            step += 1\n",
        "            \n",
        "            background.move()\n",
        "            for fire in agent.fires:\n",
        "                fire.move()\n",
        "                \n",
        "                if fire.is_hit():\n",
        "                    done = True\n",
        "                    reward = -5\n",
        "                    agent.reset()            # fighter, fire 위치 초기화\n",
        "                    \n",
        "                elif fire.is_bottom_hit():\n",
        "                    num_of_evasion += 1\n",
        "                    reward = 5\n",
        "            \n",
        "            if step == 1:                    # step이 1일때의 화면을 4장으로 넣는다.\n",
        "                state = pre_processing(gamepad)\n",
        "                history = np.stack((state, state, state, state), axis=2)\n",
        "                history = np.reshape([history], (1, 100, 80, 4)) \n",
        "            \n",
        "            # step이 1일때 가져온 첫화면 4장을 가지고 행동 선택\n",
        "            # agent.get_action을 하게 되면 fighter가 움직인다. env.step()을 한 것과 같다.\n",
        "            action = agent.get_action(history)\n",
        "            \n",
        "            \n",
        "            # step이 1을 넘긴 다음부터는 타임스텝 5단위 당 1장의 이미지만 가져와서 history에 넣는다.\n",
        "            # 5의 배수인 이미지들만 학습에 사용된다는 의미일 뿐, agent.get_action은 while문에 의해서 시온이를 계속 움직인다.\n",
        "            frame_skip += 1\n",
        "            if frame_skip % 2 == 0: \n",
        "                frame_skip = 0\n",
        "                next_state = pre_processing(gamepad)\n",
        "                next_state = np.reshape([next_state], (1, 100, 80, 1))\n",
        "                next_history = np.append(next_state, history[:, :, :, :3], axis=3)\n",
        "                \n",
        "                # 새로운 화면이 들어왔을때 기존 화면의 q_max 값을 구한다.(avg_q_max 값을 print로 보여주기 위해서)\n",
        "                agent.avg_q_max += np.amax(agent.model.predict(np.float32(history / 255.))[0]) # 모델에서 예측한 값들중 가장 큰 값을 avg_q_max에 추가한다. 왜?? 그 값이 action을 일으켰기 때문!!\n",
        "                \n",
        "                # 샘플 <s, a, r, s'>을 리플레이 메모리에 저장 후 학습\n",
        "                # frome_skip이 5의 배수인 이미지들이 순차적으로 쌓인다.\n",
        "                agent.append_sample(history, action, reward, next_history)        \n",
        "                \n",
        "                \n",
        "                # len(agent.memory)은 적어도 agent.train_start(50000)보다 커야 모델이 학습되는것 아닌가?? \n",
        "                if len(agent.memory) >= agent.train_start:\n",
        "                    agent.train_model()           \n",
        "                       \n",
        "                # 일정 시간마다 타겟모델을 모델의 가중치로 업데이트\n",
        "                if global_step % agent.update_target_rate == 0:\n",
        "                    agent.update_target_model()            \n",
        "                \n",
        "                history = next_history\n",
        "                \n",
        "            if done:\n",
        "                print(\"episode:\", e, \"num_of_evasion:\", num_of_evasion, \"memory length:\", \n",
        "                      len(agent.memory), \"epsilon:\", agent.epsilon, \"global_step:\", global_step,\n",
        "                      \"average_q:\", agent.avg_q_max / float(step), \"average_loss:\",\n",
        "                      agent.avg_loss / float(step))\n",
        "                \n",
        "                # avg_loss값을 그래프화 하기\n",
        "                average_loss_list.append(agent.avg_loss)\n",
        "                agent.avg_q_max, agent.avg_loss = 0, 0\n",
        "\n",
        "            # Google colab 버전 : 학습 모델을 episode 2000번 마다 가중치/loss 저장\n",
        "            if e % 2000 == 0: \n",
        "                agent.model.save_weights('/content/drive/My Drive/cion_eposide(50000).h5')\n",
        "\n",
        "                f = open('/content/drive/My Drive/cion_final_graph.pickle', 'wb')\n",
        "                pickle.dump(average_loss_list, f)\n",
        "                f.close()\n",
        "\n",
        "            pygame.display.update() # 게임화면 update\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_7 (Conv2D)            (None, 24, 19, 32)        8224      \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 11, 8, 64)         32832     \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 5632)              0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 256)               1442048   \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 64)                16448     \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 3)                 195       \n",
            "=================================================================\n",
            "Total params: 1,565,539\n",
            "Trainable params: 1,565,539\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_9 (Conv2D)            (None, 24, 19, 32)        8224      \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 11, 8, 64)         32832     \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 5632)              0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 256)               1442048   \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 64)                16448     \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 3)                 195       \n",
            "=================================================================\n",
            "Total params: 1,565,539\n",
            "Trainable params: 1,565,539\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "episode: 0 num_of_evasion: 0 memory length: 18 epsilon: 1.0 global_step: 37 average_q: 0.007091899584928477 average_loss: 0.0\n",
            "episode: 1 num_of_evasion: 4 memory length: 70 epsilon: 1.0 global_step: 142 average_q: 0.010047166209135736 average_loss: 0.0\n",
            "episode: 2 num_of_evasion: 3 memory length: 111 epsilon: 1.0 global_step: 225 average_q: 0.008080772934930331 average_loss: 0.0\n",
            "episode: 3 num_of_evasion: 0 memory length: 133 epsilon: 1.0 global_step: 269 average_q: 0.007331913125447251 average_loss: 0.0\n",
            "episode: 4 num_of_evasion: 10 memory length: 244 epsilon: 1.0 global_step: 492 average_q: 0.011438587866305904 average_loss: 0.0\n",
            "episode: 5 num_of_evasion: 1 memory length: 272 epsilon: 1.0 global_step: 548 average_q: 0.014147200854495168 average_loss: 0.0\n",
            "episode: 6 num_of_evasion: 1 memory length: 298 epsilon: 1.0 global_step: 600 average_q: 0.011358587494424473 average_loss: 0.0\n",
            "episode: 7 num_of_evasion: 8 memory length: 388 epsilon: 1.0 global_step: 781 average_q: 0.010283611997838508 average_loss: 0.0\n",
            "episode: 8 num_of_evasion: 12 memory length: 511 epsilon: 1.0 global_step: 1027 average_q: 0.010838774252470916 average_loss: 0.0\n",
            "episode: 9 num_of_evasion: 0 memory length: 529 epsilon: 1.0 global_step: 1063 average_q: 0.010959608247503638 average_loss: 0.0\n",
            "episode: 10 num_of_evasion: 0 memory length: 547 epsilon: 1.0 global_step: 1100 average_q: 0.009290273829891876 average_loss: 0.0\n",
            "episode: 11 num_of_evasion: 0 memory length: 570 epsilon: 1.0 global_step: 1146 average_q: 0.009444950315493928 average_loss: 0.0\n",
            "episode: 12 num_of_evasion: 2 memory length: 601 epsilon: 1.0 global_step: 1208 average_q: 0.010605924146910828 average_loss: 0.0\n",
            "episode: 13 num_of_evasion: 0 memory length: 619 epsilon: 1.0 global_step: 1244 average_q: 0.01080305443610996 average_loss: 0.0\n",
            "episode: 14 num_of_evasion: 4 memory length: 671 epsilon: 1.0 global_step: 1349 average_q: 0.009541446569242648 average_loss: 0.0\n",
            "episode: 15 num_of_evasion: 0 memory length: 689 epsilon: 1.0 global_step: 1385 average_q: 0.009353996228633655 average_loss: 0.0\n",
            "episode: 16 num_of_evasion: 8 memory length: 779 epsilon: 1.0 global_step: 1566 average_q: 0.01251605535131874 average_loss: 0.0\n",
            "episode: 17 num_of_evasion: 0 memory length: 797 epsilon: 1.0 global_step: 1602 average_q: 0.009668188208403686 average_loss: 0.0\n",
            "episode: 18 num_of_evasion: 7 memory length: 879 epsilon: 1.0 global_step: 1766 average_q: 0.010318270374782293 average_loss: 0.0\n",
            "episode: 19 num_of_evasion: 5 memory length: 940 epsilon: 1.0 global_step: 1888 average_q: 0.011389132625362302 average_loss: 0.0\n",
            "episode: 20 num_of_evasion: 0 memory length: 963 epsilon: 1.0 global_step: 1934 average_q: 0.013937718356433123 average_loss: 0.0\n",
            "episode: 21 num_of_evasion: 6 memory length: 1028 epsilon: 1.0 global_step: 2064 average_q: 0.008455740454463432 average_loss: 0.0\n",
            "episode: 22 num_of_evasion: 0 memory length: 1046 epsilon: 1.0 global_step: 2101 average_q: 0.010441095719264971 average_loss: 0.0\n",
            "episode: 23 num_of_evasion: 0 memory length: 1064 epsilon: 1.0 global_step: 2138 average_q: 0.00984759660236336 average_loss: 0.0\n",
            "episode: 24 num_of_evasion: 0 memory length: 1087 epsilon: 1.0 global_step: 2184 average_q: 0.008500933085325296 average_loss: 0.0\n",
            "episode: 25 num_of_evasion: 0 memory length: 1110 epsilon: 1.0 global_step: 2230 average_q: 0.008518731808694809 average_loss: 0.0\n",
            "episode: 26 num_of_evasion: 5 memory length: 1169 epsilon: 1.0 global_step: 2348 average_q: 0.010386948414557314 average_loss: 0.0\n",
            "episode: 27 num_of_evasion: 0 memory length: 1190 epsilon: 1.0 global_step: 2390 average_q: 0.013125292513342131 average_loss: 0.0\n",
            "episode: 28 num_of_evasion: 0 memory length: 1211 epsilon: 1.0 global_step: 2433 average_q: 0.007591482739202505 average_loss: 0.0\n",
            "episode: 29 num_of_evasion: 0 memory length: 1234 epsilon: 1.0 global_step: 2479 average_q: 0.015472738760644976 average_loss: 0.0\n",
            "episode: 30 num_of_evasion: 4 memory length: 1286 epsilon: 1.0 global_step: 2584 average_q: 0.010712637796643235 average_loss: 0.0\n",
            "episode: 31 num_of_evasion: 1 memory length: 1311 epsilon: 1.0 global_step: 2634 average_q: 0.009938491256907583 average_loss: 0.0\n",
            "episode: 32 num_of_evasion: 4 memory length: 1369 epsilon: 1.0 global_step: 2750 average_q: 0.010058028246115508 average_loss: 0.0\n",
            "episode: 33 num_of_evasion: 1 memory length: 1397 epsilon: 1.0 global_step: 2806 average_q: 0.010846312195228944 average_loss: 0.0\n",
            "episode: 34 num_of_evasion: 1 memory length: 1422 epsilon: 1.0 global_step: 2856 average_q: 0.006744020134210587 average_loss: 0.0\n",
            "episode: 35 num_of_evasion: 0 memory length: 1440 epsilon: 1.0 global_step: 2892 average_q: 0.009743741428893473 average_loss: 0.0\n",
            "episode: 36 num_of_evasion: 0 memory length: 1458 epsilon: 1.0 global_step: 2929 average_q: 0.009935357998049742 average_loss: 0.0\n",
            "episode: 37 num_of_evasion: 1 memory length: 1483 epsilon: 1.0 global_step: 2980 average_q: 0.01106823495059621 average_loss: 0.0\n",
            "episode: 38 num_of_evasion: 12 memory length: 1606 epsilon: 1.0 global_step: 3226 average_q: 0.011434323479974173 average_loss: 0.0\n",
            "episode: 39 num_of_evasion: 8 memory length: 1696 epsilon: 1.0 global_step: 3407 average_q: 0.010884589329104645 average_loss: 0.0\n",
            "episode: 40 num_of_evasion: 4 memory length: 1748 epsilon: 1.0 global_step: 3512 average_q: 0.009588462344947316 average_loss: 0.0\n",
            "episode: 41 num_of_evasion: 3 memory length: 1789 epsilon: 1.0 global_step: 3595 average_q: 0.0099817557675562 average_loss: 0.0\n",
            "episode: 42 num_of_evasion: 1 memory length: 1814 epsilon: 1.0 global_step: 3646 average_q: 0.007849460312475761 average_loss: 0.0\n",
            "episode: 43 num_of_evasion: 0 memory length: 1837 epsilon: 1.0 global_step: 3692 average_q: 0.012948732235995323 average_loss: 0.0\n",
            "episode: 44 num_of_evasion: 0 memory length: 1855 epsilon: 1.0 global_step: 3728 average_q: 0.009793736286357872 average_loss: 0.0\n",
            "episode: 45 num_of_evasion: 4 memory length: 1907 epsilon: 1.0 global_step: 3833 average_q: 0.01075223926676526 average_loss: 0.0\n",
            "episode: 46 num_of_evasion: 4 memory length: 1959 epsilon: 1.0 global_step: 3938 average_q: 0.00778890405005465 average_loss: 0.0\n",
            "episode: 47 num_of_evasion: 0 memory length: 1982 epsilon: 1.0 global_step: 3984 average_q: 0.010040642622777301 average_loss: 0.0\n",
            "episode: 48 num_of_evasion: 10 memory length: 2093 epsilon: 1.0 global_step: 4207 average_q: 0.010114584306062872 average_loss: 0.0\n",
            "episode: 49 num_of_evasion: 0 memory length: 2111 epsilon: 1.0 global_step: 4244 average_q: 0.009634741733001696 average_loss: 0.0\n",
            "episode: 50 num_of_evasion: 0 memory length: 2134 epsilon: 1.0 global_step: 4290 average_q: 0.007949449739459416 average_loss: 0.0\n",
            "episode: 51 num_of_evasion: 0 memory length: 2152 epsilon: 1.0 global_step: 4326 average_q: 0.008930314383986924 average_loss: 0.0\n",
            "episode: 52 num_of_evasion: 0 memory length: 2172 epsilon: 1.0 global_step: 4366 average_q: 0.011385442386381329 average_loss: 0.0\n",
            "episode: 53 num_of_evasion: 0 memory length: 2195 epsilon: 1.0 global_step: 4413 average_q: 0.007789188978618923 average_loss: 0.0\n",
            "episode: 54 num_of_evasion: 0 memory length: 2218 epsilon: 1.0 global_step: 4459 average_q: 0.009080092888325453 average_loss: 0.0\n",
            "episode: 55 num_of_evasion: 4 memory length: 2275 epsilon: 1.0 global_step: 4574 average_q: 0.009684305289841216 average_loss: 0.0\n",
            "episode: 56 num_of_evasion: 7 memory length: 2360 epsilon: 1.0 global_step: 4745 average_q: 0.012091973594291821 average_loss: 0.0\n",
            "episode: 57 num_of_evasion: 1 memory length: 2384 epsilon: 1.0 global_step: 4794 average_q: 0.008212661104542869 average_loss: 0.0\n",
            "episode: 58 num_of_evasion: 1 memory length: 2409 epsilon: 1.0 global_step: 4845 average_q: 0.00783129257843921 average_loss: 0.0\n",
            "episode: 59 num_of_evasion: 0 memory length: 2427 epsilon: 1.0 global_step: 4881 average_q: 0.008407547643097738 average_loss: 0.0\n",
            "episode: 60 num_of_evasion: 0 memory length: 2450 epsilon: 1.0 global_step: 4927 average_q: 0.008437420950149712 average_loss: 0.0\n",
            "episode: 61 num_of_evasion: 0 memory length: 2473 epsilon: 1.0 global_step: 4973 average_q: 0.014198424193360235 average_loss: 0.0\n",
            "episode: 62 num_of_evasion: 0 memory length: 2491 epsilon: 1.0 global_step: 5009 average_q: 0.009927643463015556 average_loss: 0.0\n",
            "episode: 63 num_of_evasion: 4 memory length: 2543 epsilon: 1.0 global_step: 5114 average_q: 0.013558222557462397 average_loss: 0.0\n",
            "episode: 64 num_of_evasion: 1 memory length: 2568 epsilon: 1.0 global_step: 5164 average_q: 0.007964139971882105 average_loss: 0.0\n",
            "episode: 65 num_of_evasion: 6 memory length: 2633 epsilon: 1.0 global_step: 5294 average_q: 0.012720325553359894 average_loss: 0.0\n",
            "episode: 66 num_of_evasion: 6 memory length: 2699 epsilon: 1.0 global_step: 5426 average_q: 0.009392638902433893 average_loss: 0.0\n",
            "episode: 67 num_of_evasion: 3 memory length: 2741 epsilon: 1.0 global_step: 5510 average_q: 0.010842601125616403 average_loss: 0.0\n",
            "episode: 68 num_of_evasion: 1 memory length: 2766 epsilon: 1.0 global_step: 5561 average_q: 0.007451981748910803 average_loss: 0.0\n",
            "episode: 69 num_of_evasion: 0 memory length: 2784 epsilon: 1.0 global_step: 5597 average_q: 0.007582216433042454 average_loss: 0.0\n",
            "episode: 70 num_of_evasion: 0 memory length: 2806 epsilon: 1.0 global_step: 5641 average_q: 0.009258714673871344 average_loss: 0.0\n",
            "episode: 71 num_of_evasion: 1 memory length: 2831 epsilon: 1.0 global_step: 5692 average_q: 0.008460543008849901 average_loss: 0.0\n",
            "episode: 72 num_of_evasion: 8 memory length: 2919 epsilon: 1.0 global_step: 5869 average_q: 0.009282483811369218 average_loss: 0.0\n",
            "episode: 73 num_of_evasion: 3 memory length: 2961 epsilon: 1.0 global_step: 5953 average_q: 0.00982647579041354 average_loss: 0.0\n",
            "episode: 74 num_of_evasion: 3 memory length: 3004 epsilon: 1.0 global_step: 6039 average_q: 0.010480158428964747 average_loss: 0.0\n",
            "episode: 75 num_of_evasion: 0 memory length: 3025 epsilon: 1.0 global_step: 6081 average_q: 0.011874588960338207 average_loss: 0.0\n",
            "episode: 76 num_of_evasion: 4 memory length: 3083 epsilon: 1.0 global_step: 6197 average_q: 0.010230106594651166 average_loss: 0.0\n",
            "episode: 77 num_of_evasion: 0 memory length: 3101 epsilon: 1.0 global_step: 6233 average_q: 0.009238727985777788 average_loss: 0.0\n",
            "episode: 78 num_of_evasion: 13 memory length: 3237 epsilon: 1.0 global_step: 6505 average_q: 0.008734995753483792 average_loss: 0.0\n",
            "episode: 79 num_of_evasion: 0 memory length: 3255 epsilon: 1.0 global_step: 6542 average_q: 0.008341455832123756 average_loss: 0.0\n",
            "episode: 80 num_of_evasion: 6 memory length: 3320 epsilon: 1.0 global_step: 6673 average_q: 0.010533163073394476 average_loss: 0.0\n",
            "episode: 81 num_of_evasion: 2 memory length: 3352 epsilon: 1.0 global_step: 6738 average_q: 0.011538586077781824 average_loss: 0.0\n",
            "episode: 82 num_of_evasion: 0 memory length: 3370 epsilon: 1.0 global_step: 6774 average_q: 0.007632164685573015 average_loss: 0.0\n",
            "episode: 83 num_of_evasion: 1 memory length: 3395 epsilon: 1.0 global_step: 6825 average_q: 0.006147958472937199 average_loss: 0.0\n",
            "episode: 84 num_of_evasion: 1 memory length: 3420 epsilon: 1.0 global_step: 6876 average_q: 0.013138591494484275 average_loss: 0.0\n",
            "episode: 85 num_of_evasion: 0 memory length: 3443 epsilon: 1.0 global_step: 6922 average_q: 0.007073355696933425 average_loss: 0.0\n",
            "episode: 86 num_of_evasion: 0 memory length: 3461 epsilon: 1.0 global_step: 6958 average_q: 0.009519941432194578 average_loss: 0.0\n",
            "episode: 87 num_of_evasion: 4 memory length: 3513 epsilon: 1.0 global_step: 7063 average_q: 0.011275828773865388 average_loss: 0.0\n",
            "episode: 88 num_of_evasion: 3 memory length: 3554 epsilon: 1.0 global_step: 7146 average_q: 0.007519713047532791 average_loss: 0.0\n",
            "episode: 89 num_of_evasion: 6 memory length: 3619 epsilon: 1.0 global_step: 7277 average_q: 0.011455798643385237 average_loss: 0.0\n",
            "episode: 90 num_of_evasion: 4 memory length: 3677 epsilon: 1.0 global_step: 7393 average_q: 0.013105009859106663 average_loss: 0.0\n",
            "episode: 91 num_of_evasion: 0 memory length: 3695 epsilon: 1.0 global_step: 7430 average_q: 0.009058282395975815 average_loss: 0.0\n",
            "episode: 92 num_of_evasion: 3 memory length: 3737 epsilon: 1.0 global_step: 7514 average_q: 0.012862384952660207 average_loss: 0.0\n",
            "episode: 93 num_of_evasion: 1 memory length: 3762 epsilon: 1.0 global_step: 7565 average_q: 0.012036688105367562 average_loss: 0.0\n",
            "episode: 94 num_of_evasion: 1 memory length: 3787 epsilon: 1.0 global_step: 7616 average_q: 0.009301109568161122 average_loss: 0.0\n",
            "episode: 95 num_of_evasion: 1 memory length: 3812 epsilon: 1.0 global_step: 7667 average_q: 0.009040354408652466 average_loss: 0.0\n",
            "episode: 96 num_of_evasion: 0 memory length: 3830 epsilon: 1.0 global_step: 7704 average_q: 0.007697629780080673 average_loss: 0.0\n",
            "episode: 97 num_of_evasion: 6 memory length: 3895 epsilon: 1.0 global_step: 7835 average_q: 0.010000102708955075 average_loss: 0.0\n",
            "episode: 98 num_of_evasion: 0 memory length: 3918 epsilon: 1.0 global_step: 7881 average_q: 0.009379912800479518 average_loss: 0.0\n",
            "episode: 99 num_of_evasion: 4 memory length: 3976 epsilon: 1.0 global_step: 7997 average_q: 0.012159331051375845 average_loss: 0.0\n",
            "episode: 100 num_of_evasion: 0 memory length: 3999 epsilon: 1.0 global_step: 8043 average_q: 0.010827878874767086 average_loss: 0.0\n",
            "episode: 101 num_of_evasion: 8 memory length: 4088 epsilon: 1.0 global_step: 8222 average_q: 0.010119194125850274 average_loss: 0.0\n",
            "episode: 102 num_of_evasion: 0 memory length: 4106 epsilon: 1.0 global_step: 8259 average_q: 0.008686692013430435 average_loss: 0.0\n",
            "episode: 103 num_of_evasion: 0 memory length: 4125 epsilon: 1.0 global_step: 8297 average_q: 0.009975964227985395 average_loss: 0.0\n",
            "episode: 104 num_of_evasion: 0 memory length: 4143 epsilon: 1.0 global_step: 8334 average_q: 0.008250879287417676 average_loss: 0.0\n",
            "episode: 105 num_of_evasion: 0 memory length: 4161 epsilon: 1.0 global_step: 8371 average_q: 0.016908846594191885 average_loss: 0.0\n",
            "episode: 106 num_of_evasion: 6 memory length: 4228 epsilon: 1.0 global_step: 8505 average_q: 0.00860794038692517 average_loss: 0.0\n",
            "episode: 107 num_of_evasion: 0 memory length: 4246 epsilon: 1.0 global_step: 8542 average_q: 0.006055321890508404 average_loss: 0.0\n",
            "episode: 108 num_of_evasion: 8 memory length: 4335 epsilon: 1.0 global_step: 8720 average_q: 0.0121585290978422 average_loss: 0.0\n",
            "episode: 109 num_of_evasion: 2 memory length: 4366 epsilon: 1.0 global_step: 8783 average_q: 0.011837893971315926 average_loss: 0.0\n",
            "episode: 110 num_of_evasion: 0 memory length: 4389 epsilon: 1.0 global_step: 8829 average_q: 0.008118575707118473 average_loss: 0.0\n",
            "episode: 111 num_of_evasion: 1 memory length: 4415 epsilon: 1.0 global_step: 8881 average_q: 0.010541828026851783 average_loss: 0.0\n",
            "episode: 112 num_of_evasion: 1 memory length: 4444 epsilon: 1.0 global_step: 8939 average_q: 0.009056873177981069 average_loss: 0.0\n",
            "episode: 113 num_of_evasion: 0 memory length: 4467 epsilon: 1.0 global_step: 8985 average_q: 0.008899960261972054 average_loss: 0.0\n",
            "episode: 114 num_of_evasion: 8 memory length: 4557 epsilon: 1.0 global_step: 9165 average_q: 0.01137538838148531 average_loss: 0.0\n",
            "episode: 115 num_of_evasion: 4 memory length: 4609 epsilon: 1.0 global_step: 9270 average_q: 0.009049412098136686 average_loss: 0.0\n",
            "episode: 116 num_of_evasion: 0 memory length: 4627 epsilon: 1.0 global_step: 9306 average_q: 0.01142506488961064 average_loss: 0.0\n",
            "episode: 117 num_of_evasion: 1 memory length: 4652 epsilon: 1.0 global_step: 9357 average_q: 0.012288519053482543 average_loss: 0.0\n",
            "episode: 118 num_of_evasion: 0 memory length: 4672 epsilon: 1.0 global_step: 9397 average_q: 0.009227378503419459 average_loss: 0.0\n",
            "episode: 119 num_of_evasion: 0 memory length: 4695 epsilon: 1.0 global_step: 9443 average_q: 0.011022588744273653 average_loss: 0.0\n",
            "episode: 120 num_of_evasion: 5 memory length: 4758 epsilon: 1.0 global_step: 9569 average_q: 0.009981361907049423 average_loss: 0.0\n",
            "episode: 121 num_of_evasion: 0 memory length: 4776 epsilon: 1.0 global_step: 9605 average_q: 0.00914940162975755 average_loss: 0.0\n",
            "episode: 122 num_of_evasion: 0 memory length: 4797 epsilon: 1.0 global_step: 9647 average_q: 0.0077651671488725 average_loss: 0.0\n",
            "episode: 123 num_of_evasion: 18 memory length: 4985 epsilon: 1.0 global_step: 10023 average_q: 0.01170130151675697 average_loss: 0.0\n",
            "episode: 124 num_of_evasion: 3 memory length: 5026 epsilon: 1.0 global_step: 10106 average_q: 0.010773318819701672 average_loss: 0.0\n",
            "episode: 125 num_of_evasion: 0 memory length: 5049 epsilon: 1.0 global_step: 10152 average_q: 0.007727839801784443 average_loss: 0.0\n",
            "episode: 126 num_of_evasion: 1 memory length: 5075 epsilon: 1.0 global_step: 10204 average_q: 0.010914310705489837 average_loss: 0.0\n",
            "episode: 127 num_of_evasion: 1 memory length: 5104 epsilon: 1.0 global_step: 10262 average_q: 0.010461211493559953 average_loss: 0.0\n",
            "episode: 128 num_of_evasion: 0 memory length: 5123 epsilon: 1.0 global_step: 10301 average_q: 0.008924052390890816 average_loss: 0.0\n",
            "episode: 129 num_of_evasion: 0 memory length: 5146 epsilon: 1.0 global_step: 10347 average_q: 0.01162816389747288 average_loss: 0.0\n",
            "episode: 130 num_of_evasion: 0 memory length: 5167 epsilon: 1.0 global_step: 10390 average_q: 0.005833592612382977 average_loss: 0.0\n",
            "episode: 131 num_of_evasion: 0 memory length: 5187 epsilon: 1.0 global_step: 10431 average_q: 0.00788560546451907 average_loss: 0.0\n",
            "episode: 132 num_of_evasion: 0 memory length: 5205 epsilon: 1.0 global_step: 10468 average_q: 0.009192193203882591 average_loss: 0.0\n",
            "episode: 133 num_of_evasion: 4 memory length: 5260 epsilon: 1.0 global_step: 10578 average_q: 0.00950701709159396 average_loss: 0.0\n",
            "episode: 134 num_of_evasion: 0 memory length: 5283 epsilon: 1.0 global_step: 10624 average_q: 0.01050797712219798 average_loss: 0.0\n",
            "episode: 135 num_of_evasion: 0 memory length: 5301 epsilon: 1.0 global_step: 10661 average_q: 0.012223293123817121 average_loss: 0.0\n",
            "episode: 136 num_of_evasion: 0 memory length: 5319 epsilon: 1.0 global_step: 10698 average_q: 0.00775886289271954 average_loss: 0.0\n",
            "episode: 137 num_of_evasion: 0 memory length: 5342 epsilon: 1.0 global_step: 10744 average_q: 0.01078035370649203 average_loss: 0.0\n",
            "episode: 138 num_of_evasion: 0 memory length: 5360 epsilon: 1.0 global_step: 10780 average_q: 0.00877920623558263 average_loss: 0.0\n",
            "episode: 139 num_of_evasion: 4 memory length: 5412 epsilon: 1.0 global_step: 10885 average_q: 0.009824785044682878 average_loss: 0.0\n",
            "episode: 140 num_of_evasion: 7 memory length: 5497 epsilon: 1.0 global_step: 11055 average_q: 0.009797472958042123 average_loss: 0.0\n",
            "episode: 141 num_of_evasion: 1 memory length: 5522 epsilon: 1.0 global_step: 11105 average_q: 0.010914459619671106 average_loss: 0.0\n",
            "episode: 142 num_of_evasion: 13 memory length: 5660 epsilon: 1.0 global_step: 11382 average_q: 0.0114668708888054 average_loss: 0.0\n",
            "episode: 143 num_of_evasion: 4 memory length: 5712 epsilon: 1.0 global_step: 11487 average_q: 0.009784564717362325 average_loss: 0.0\n",
            "episode: 144 num_of_evasion: 17 memory length: 5882 epsilon: 1.0 global_step: 11828 average_q: 0.011391397889927466 average_loss: 0.0\n",
            "episode: 145 num_of_evasion: 1 memory length: 5909 epsilon: 1.0 global_step: 11883 average_q: 0.011999520295384255 average_loss: 0.0\n",
            "episode: 146 num_of_evasion: 14 memory length: 6050 epsilon: 1.0 global_step: 12165 average_q: 0.009369071081762883 average_loss: 0.0\n",
            "episode: 147 num_of_evasion: 3 memory length: 6091 epsilon: 1.0 global_step: 12248 average_q: 0.011269974996369466 average_loss: 0.0\n",
            "episode: 148 num_of_evasion: 8 memory length: 6180 epsilon: 1.0 global_step: 12426 average_q: 0.010396357641419333 average_loss: 0.0\n",
            "episode: 149 num_of_evasion: 7 memory length: 6262 epsilon: 1.0 global_step: 12590 average_q: 0.011628213298802332 average_loss: 0.0\n",
            "episode: 150 num_of_evasion: 0 memory length: 6280 epsilon: 1.0 global_step: 12626 average_q: 0.007523893313999806 average_loss: 0.0\n",
            "episode: 151 num_of_evasion: 4 memory length: 6332 epsilon: 1.0 global_step: 12731 average_q: 0.008099547518594635 average_loss: 0.0\n",
            "episode: 152 num_of_evasion: 0 memory length: 6350 epsilon: 1.0 global_step: 12768 average_q: 0.009214537520263646 average_loss: 0.0\n",
            "episode: 153 num_of_evasion: 0 memory length: 6368 epsilon: 1.0 global_step: 12805 average_q: 0.010430939143171182 average_loss: 0.0\n",
            "episode: 154 num_of_evasion: 4 memory length: 6426 epsilon: 1.0 global_step: 12921 average_q: 0.010313786923532086 average_loss: 0.0\n",
            "episode: 155 num_of_evasion: 0 memory length: 6449 epsilon: 1.0 global_step: 12967 average_q: 0.01288727192086694 average_loss: 0.0\n",
            "episode: 156 num_of_evasion: 8 memory length: 6537 epsilon: 1.0 global_step: 13144 average_q: 0.01049511055304211 average_loss: 0.0\n",
            "episode: 157 num_of_evasion: 1 memory length: 6563 epsilon: 1.0 global_step: 13197 average_q: 0.010148011850860884 average_loss: 0.0\n",
            "episode: 158 num_of_evasion: 10 memory length: 6675 epsilon: 1.0 global_step: 13421 average_q: 0.010401531596601541 average_loss: 0.0\n",
            "episode: 159 num_of_evasion: 1 memory length: 6700 epsilon: 1.0 global_step: 13472 average_q: 0.01011497666146241 average_loss: 0.0\n",
            "episode: 160 num_of_evasion: 1 memory length: 6724 epsilon: 1.0 global_step: 13521 average_q: 0.008744386480931116 average_loss: 0.0\n",
            "episode: 161 num_of_evasion: 1 memory length: 6749 epsilon: 1.0 global_step: 13571 average_q: 0.013164256680756808 average_loss: 0.0\n",
            "episode: 162 num_of_evasion: 0 memory length: 6767 epsilon: 1.0 global_step: 13607 average_q: 0.008076713876410698 average_loss: 0.0\n",
            "episode: 163 num_of_evasion: 3 memory length: 6809 epsilon: 1.0 global_step: 13691 average_q: 0.011372197203205101 average_loss: 0.0\n",
            "episode: 164 num_of_evasion: 4 memory length: 6867 epsilon: 1.0 global_step: 13808 average_q: 0.011443210901039788 average_loss: 0.0\n",
            "episode: 165 num_of_evasion: 0 memory length: 6890 epsilon: 1.0 global_step: 13854 average_q: 0.010414190069043441 average_loss: 0.0\n",
            "episode: 166 num_of_evasion: 3 memory length: 6936 epsilon: 1.0 global_step: 13946 average_q: 0.009969137677335706 average_loss: 0.0\n",
            "episode: 167 num_of_evasion: 7 memory length: 7022 epsilon: 1.0 global_step: 14119 average_q: 0.010856051598217963 average_loss: 0.0\n",
            "episode: 168 num_of_evasion: 4 memory length: 7079 epsilon: 1.0 global_step: 14234 average_q: 0.007866698867924835 average_loss: 0.0\n",
            "episode: 169 num_of_evasion: 0 memory length: 7097 epsilon: 1.0 global_step: 14270 average_q: 0.00623105463778807 average_loss: 0.0\n",
            "episode: 170 num_of_evasion: 0 memory length: 7116 epsilon: 1.0 global_step: 14308 average_q: 0.009724507083822238 average_loss: 0.0\n",
            "episode: 171 num_of_evasion: 0 memory length: 7135 epsilon: 1.0 global_step: 14346 average_q: 0.010058349390563211 average_loss: 0.0\n",
            "episode: 172 num_of_evasion: 0 memory length: 7158 epsilon: 1.0 global_step: 14392 average_q: 0.009160907594891994 average_loss: 0.0\n",
            "episode: 173 num_of_evasion: 0 memory length: 7181 epsilon: 1.0 global_step: 14438 average_q: 0.008338658726247757 average_loss: 0.0\n",
            "episode: 174 num_of_evasion: 0 memory length: 7199 epsilon: 1.0 global_step: 14474 average_q: 0.011185442097485065 average_loss: 0.0\n",
            "episode: 175 num_of_evasion: 1 memory length: 7224 epsilon: 1.0 global_step: 14525 average_q: 0.011618152808617143 average_loss: 0.0\n",
            "episode: 176 num_of_evasion: 0 memory length: 7242 epsilon: 1.0 global_step: 14561 average_q: 0.00872078318045371 average_loss: 0.0\n",
            "episode: 177 num_of_evasion: 0 memory length: 7260 epsilon: 1.0 global_step: 14598 average_q: 0.011529898573015188 average_loss: 0.0\n",
            "episode: 178 num_of_evasion: 0 memory length: 7278 epsilon: 1.0 global_step: 14635 average_q: 0.010035516669017237 average_loss: 0.0\n",
            "episode: 179 num_of_evasion: 7 memory length: 7360 epsilon: 1.0 global_step: 14799 average_q: 0.01034270478384115 average_loss: 0.0\n",
            "episode: 180 num_of_evasion: 1 memory length: 7385 epsilon: 1.0 global_step: 14849 average_q: 0.01225533651188016 average_loss: 0.0\n",
            "episode: 181 num_of_evasion: 4 memory length: 7443 epsilon: 1.0 global_step: 14965 average_q: 0.007579680036448328 average_loss: 0.0\n",
            "episode: 182 num_of_evasion: 0 memory length: 7465 epsilon: 1.0 global_step: 15009 average_q: 0.009858991516838696 average_loss: 0.0\n",
            "episode: 183 num_of_evasion: 4 memory length: 7517 epsilon: 1.0 global_step: 15114 average_q: 0.011761184919270732 average_loss: 0.0\n",
            "episode: 184 num_of_evasion: 7 memory length: 7599 epsilon: 1.0 global_step: 15278 average_q: 0.013373969128446244 average_loss: 0.0\n",
            "episode: 185 num_of_evasion: 1 memory length: 7624 epsilon: 1.0 global_step: 15329 average_q: 0.006642411449266707 average_loss: 0.0\n",
            "episode: 186 num_of_evasion: 0 memory length: 7642 epsilon: 1.0 global_step: 15365 average_q: 0.009475946413456567 average_loss: 0.0\n",
            "episode: 187 num_of_evasion: 0 memory length: 7665 epsilon: 1.0 global_step: 15411 average_q: 0.01246612099930644 average_loss: 0.0\n",
            "episode: 188 num_of_evasion: 4 memory length: 7722 epsilon: 1.0 global_step: 15526 average_q: 0.009754978229656168 average_loss: 0.0\n",
            "episode: 189 num_of_evasion: 1 memory length: 7747 epsilon: 1.0 global_step: 15577 average_q: 0.008484413523190454 average_loss: 0.0\n",
            "episode: 190 num_of_evasion: 0 memory length: 7765 epsilon: 1.0 global_step: 15613 average_q: 0.00845610189329212 average_loss: 0.0\n",
            "episode: 191 num_of_evasion: 4 memory length: 7823 epsilon: 1.0 global_step: 15729 average_q: 0.009990440095485798 average_loss: 0.0\n",
            "episode: 192 num_of_evasion: 1 memory length: 7848 epsilon: 1.0 global_step: 15780 average_q: 0.011557590837279955 average_loss: 0.0\n",
            "episode: 193 num_of_evasion: 0 memory length: 7866 epsilon: 1.0 global_step: 15816 average_q: 0.006767390580433939 average_loss: 0.0\n",
            "episode: 194 num_of_evasion: 3 memory length: 7908 epsilon: 1.0 global_step: 15900 average_q: 0.009116861880535171 average_loss: 0.0\n",
            "episode: 195 num_of_evasion: 8 memory length: 7997 epsilon: 1.0 global_step: 16078 average_q: 0.012799596602327368 average_loss: 0.0\n",
            "episode: 196 num_of_evasion: 6 memory length: 8062 epsilon: 0.9943299999999978 global_step: 16209 average_q: 0.10850191417768712 average_loss: 1.0135743099314567\n",
            "episode: 197 num_of_evasion: 0 memory length: 8084 epsilon: 0.9923499999999971 global_step: 16253 average_q: 0.4231744469566779 average_loss: 0.9781562008640983\n",
            "episode: 198 num_of_evasion: 18 memory length: 8272 epsilon: 0.9754299999999906 global_step: 16629 average_q: 1.3887339630263282 average_loss: 0.6145642176270485\n",
            "episode: 199 num_of_evasion: 0 memory length: 8290 epsilon: 0.97380999999999 global_step: 16665 average_q: 0.7384260037603477 average_loss: 0.4420485513077842\n",
            "episode: 200 num_of_evasion: 0 memory length: 8308 epsilon: 0.9721899999999893 global_step: 16702 average_q: 0.17516143534433198 average_loss: 0.395617878114855\n",
            "episode: 201 num_of_evasion: 12 memory length: 8431 epsilon: 0.9611199999999851 global_step: 16948 average_q: 1.9236187403415883 average_loss: 0.3084654146578254\n",
            "episode: 202 num_of_evasion: 4 memory length: 8483 epsilon: 0.9564399999999833 global_step: 17053 average_q: 1.4687059367696444 average_loss: 0.21924965268089658\n",
            "episode: 203 num_of_evasion: 0 memory length: 8506 epsilon: 0.9543699999999825 global_step: 17099 average_q: 0.16608126639671947 average_loss: 0.2018535817446916\n",
            "episode: 204 num_of_evasion: 0 memory length: 8524 epsilon: 0.9527499999999819 global_step: 17136 average_q: 0.010077274681345836 average_loss: 0.1878153029325846\n",
            "episode: 205 num_of_evasion: 0 memory length: 8547 epsilon: 0.9506799999999811 global_step: 17182 average_q: 0.11955915942140248 average_loss: 0.1894017334865487\n",
            "episode: 206 num_of_evasion: 1 memory length: 8573 epsilon: 0.9483399999999802 global_step: 17234 average_q: 0.7893121780899282 average_loss: 0.17797023745683524\n",
            "episode: 207 num_of_evasion: 1 memory length: 8597 epsilon: 0.9461799999999794 global_step: 17283 average_q: 0.3804900610097209 average_loss: 0.16060226182548368\n",
            "episode: 208 num_of_evasion: 8 memory length: 8685 epsilon: 0.9382599999999763 global_step: 17460 average_q: 1.6601954664677212 average_loss: 0.15546858243349582\n",
            "episode: 209 num_of_evasion: 1 memory length: 8710 epsilon: 0.9360099999999755 global_step: 17511 average_q: 0.23627791265208348 average_loss: 0.14703343574907266\n",
            "episode: 210 num_of_evasion: 15 memory length: 8870 epsilon: 0.92160999999997 global_step: 17832 average_q: 2.04183339336858 average_loss: 0.1325153176008355\n",
            "episode: 211 num_of_evasion: 3 memory length: 8912 epsilon: 0.9178299999999685 global_step: 17917 average_q: 1.1502981304684106 average_loss: 0.12014977125560536\n",
            "episode: 212 num_of_evasion: 7 memory length: 8996 epsilon: 0.9102699999999656 global_step: 18085 average_q: 1.8126736567910051 average_loss: 0.1147239984323581\n",
            "episode: 213 num_of_evasion: 0 memory length: 9019 epsilon: 0.9081999999999648 global_step: 18131 average_q: 0.07648352938501732 average_loss: 0.1147815105707749\n",
            "episode: 214 num_of_evasion: 3 memory length: 9061 epsilon: 0.9044199999999634 global_step: 18215 average_q: 1.346291699136297 average_loss: 0.10842863363879067\n",
            "episode: 215 num_of_evasion: 0 memory length: 9079 epsilon: 0.9027999999999627 global_step: 18252 average_q: -0.0035555228288914704 average_loss: 0.10947425864838264\n",
            "episode: 216 num_of_evasion: 5 memory length: 9138 epsilon: 0.8974899999999607 global_step: 18370 average_q: 1.5987034521807553 average_loss: 0.10689991908305782\n",
            "episode: 217 num_of_evasion: 0 memory length: 9156 epsilon: 0.8958699999999601 global_step: 18406 average_q: 0.04681483997652928 average_loss: 0.10005639452073309\n",
            "episode: 218 num_of_evasion: 0 memory length: 9174 epsilon: 0.8942499999999595 global_step: 18443 average_q: -0.05562264280947479 average_loss: 0.08545497704196621\n",
            "episode: 219 num_of_evasion: 15 memory length: 9330 epsilon: 0.8802099999999541 global_step: 18755 average_q: 2.055685357405589 average_loss: 0.09863514885401879\n",
            "episode: 220 num_of_evasion: 3 memory length: 9371 epsilon: 0.8765199999999527 global_step: 18838 average_q: 1.1646328383764948 average_loss: 0.09077136766120612\n",
            "episode: 221 num_of_evasion: 1 memory length: 9396 epsilon: 0.8742699999999518 global_step: 18889 average_q: 0.26589001163694204 average_loss: 0.09253246643963982\n",
            "episode: 222 num_of_evasion: 8 memory length: 9484 epsilon: 0.8663499999999488 global_step: 19066 average_q: 1.7938312153821274 average_loss: 0.08335399152027012\n",
            "episode: 223 num_of_evasion: 0 memory length: 9502 epsilon: 0.8647299999999482 global_step: 19103 average_q: -0.017383685285175168 average_loss: 0.08611725431841773\n",
            "episode: 224 num_of_evasion: 10 memory length: 9613 epsilon: 0.8547399999999443 global_step: 19326 average_q: 1.9167554716625557 average_loss: 0.08834041080400014\n",
            "episode: 225 num_of_evasion: 3 memory length: 9654 epsilon: 0.8510499999999429 global_step: 19409 average_q: 1.1367625170443432 average_loss: 0.08277321238833737\n",
            "episode: 226 num_of_evasion: 0 memory length: 9672 epsilon: 0.8494299999999423 global_step: 19446 average_q: 0.13388213495144974 average_loss: 0.08681442588567734\n",
            "episode: 227 num_of_evasion: 4 memory length: 9730 epsilon: 0.8442099999999403 global_step: 19562 average_q: 1.5347054060709118 average_loss: 0.07861132611488474\n",
            "episode: 228 num_of_evasion: 1 memory length: 9759 epsilon: 0.8415999999999393 global_step: 19621 average_q: 0.5994157701162464 average_loss: 0.0776385584120023\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}