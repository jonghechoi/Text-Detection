########## 8장. 영상 매칭과 추적
##### 8.1 비슷한 그림 찾기
### 8.1.1 평균 해시 매칭
import cv2

img = cv2.imread('pistol.jpg')
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

gray = cv2.resize(gray, (16, 16))

avg = gray.mean()

bin = 1 * (gray > avg)


# 2진수 문자열을 16진수 문자열로 변환
'''
10진수가 아닌 다른 진수 형태를 표현하려면 숫자 앞에 접두어를 붙여준다.
- 2진수: 0b
- 8진수: 0
- 16진수: 0x

10진수를 다른 진수로 바꾸려면 아래의 함수들을 사용한다.
- 10진수 -> 2진수 : bin()
- 10진수 -> 8진수 : oct()
- 10진수 -> 16진수 : hex()

다른 진수의 문자열을 숫자형으로 변환하려면 int()를 사용한다.
- int('0b101010', 2)
- int('0o52', 8)
- int('0x2a', 16)
위의 식은 모두 42라는 같은 값을 출력한다.g

'''
dhash = []
for row in bin.tolist():
    s = ''.join([str(i) for i in row])
    dhash.append( '%x'%(int(s,2)) )
dhash = ''.join(dhash)
print(dhash)

cv2.imshow('pistol', img)
cv2.waitKey(0)
cv2.destroyAllWindows()

##### 8.2 영상의 특징과 키 포인트
### 8.2.1 코너 특징 검출 (해리스 코너)
import cv2, numpy as np

img = cv2.imread('house.jpg')
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

corner = cv2.cornerHarris(gray, 2, 3, 0.04)

coord = np.where(corner > 0.1*corner.max())
coord = np.stack((coord[1], coord[0]), axis=-1)o98yuu7
    
for x, y in coord:
    cv2.circle(img, (x,y), 5, (0,0,255), 1)

corner_norm = cv2.normalize( corner, None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8U)
corner_norm = cv2.cvtColor(corner_norm, cv2.COLOR_GRAY2BGR)

merged = np.hstack((corner_norm, img))

cv2.imshow('merged', merged)
cv2.waitKey(0)
cv2.destroyAllWindows()
    
    
### 8.2.2 키 포인트와 특징 검출기
# goodFeaturesToTrack()
import cv2, numpy as np

img = cv2.imread('house.jpg')
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    
corners = cv2.goodFeaturesToTrack(gray, 80, 0.05, 10)
    
corners = np.int32(corners)
    
for corner in corners:
    x, y = corner[0]
    cv2.circle(img, (x,y), 5, (0,0,255), 1)    
    
cv2.imshow('house', img)
cv2.waitKey(0)
cv2.destroyAllWindows()
        
    
    
    
### 8.2.3 GFTTDETECTOR
import cv2, numpy as np

img = cv2.imread('house.jpg')
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

gftt = cv2.GFTTDetector_create()
    
keypoints = gftt.detect(gray, None)

img_draw = cv2.drawKeypoints(img, keypoints, None)

cv2.imshow('GFTTDetector', img_draw)
cv2.waitKey(0)
cv2.destroyAllWindows()
    
    
### 8.2.4 FAST (Feature from Accelerated Segment Test)
import cv2, numpy as np

img = cv2.imread('house.jpg')
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

fast = cv2.FastFeatureDetector_create(50)
    
keypoints = fast.detect(gray, None)

img_draw = cv2.drawKeypoints(img, keypoints, None)

cv2.imshow('GFTTDetector', img_draw)
cv2.waitKey(0)
cv2.destroyAllWindows()
    
    
### 8.2.5 SimpleBlobDetector (Binary Large Object)
import cv2, numpy as np

img = cv2.imread('house.jpg')
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

detector = cv2.SimpleBlobDetector_create()
    
keypoints = detector.detect(gray, None)

img_draw = cv2.drawKeypoints(img, keypoints, None, (0,0,255), flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)

cv2.imshow('GFTTDetector', img_draw)
cv2.waitKey(0)
cv2.destroyAllWindows()  
    
    
    
##### 디스크립터 추출기
### 8.3.1 특징 디스크립터와 추출기 
# SIFT와 SURF 는 특허권 때문에 사용하지 못하기 때문에 ORB를 사용한다.
import cv2, numpy as np

img = cv2.imread('house.jpg')
img = cv2.resize(img, (800,600))
gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

orb = cv2.ORB_create()

keypoints, descriptor = orb.detectAndCompute(img, None)

img_draw = cv2.drawKeypoints(img, keypoints, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)

cv2.imshow('ORB', img_draw)
cv2.waitKey(0)
cv2.destroyAllWindows() 



### 8.4.1 특징 매칭 인터페이스

matcher = cv2.DescriptorMatcher_create(matcherType)
matches = matcher.match(queryDescriptors, trainDescriptors)



### 8.4.2 BFMatcher
# SIFT와 SURF 는 특허권 때문에 사용하지 못하기 때문에 ORB를 사용한다.
import cv2, numpy as np

img1 = cv2.imread('taekwonv1.jpg')
img2 = cv2.imread('figures.jpg')

gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)
gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)

detector = cv2.ORB_create() # SIFT와 SURF를 쓰고자 한다면 이 부분만 cv2.xfeatures2d.SIFT_create(), cv2.xfeatures2d.SURF_create() 

kp1, desc1 = detector.detectAndCompute(gray1, None)
kp2, desc2 = detector.detectAndCompute(gray2, None)

matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck = True)

matches = matcher.match(desc1, desc2)

res = cv2.drawMatches(img1, kp1, img2, kp2, matches, None, (0,0,255), flags = cv2.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS)

cv2.imshow('MFMatcher + ORB', res)
cv2.waitKey(0)
cv2.destroyAllWindows() 



### 8.4.3 FLANN (특징 매칭기 2개중 한개)
# SIFT와 SURF 는 특허권 때문에 사용하지 못하기 때문에 ORB를 사용한다.
import cv2, numpy as np

img1 = cv2.imread('taekwonv1.jpg')
img2 = cv2.imread('figures.jpg')

gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)
gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)

detector = cv2.ORB_create() # SIFT와 SURF를 쓰고자 한다면 이 부분만 cv2.xfeatures2d.SIFT_create(), cv2.xfeatures2d.SURF_create() 

kp1, desc1 = detector.detectAndCompute(gray1, None)
kp2, desc2 = detector.detectAndCompute(gray2, None)

FLANN_INDEX_LSH = 6
index_params = dict(algorithm = FLANN_INDEX_LSH,
                    talbe_number = 6,
                    key_size = 12,
                    multi_probe_level = 1)

search_params = dict(checks = 32)

matcher = cv2.FlannBasedMatcher(index_params, search_params)

matches = matcher.match(desc1, desc2)

res = cv2.drawMatches(img1, kp1, img2, kp2, matches, None, flags = cv2.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS)

cv2.imshow('Flann + ORB', res)
cv2.waitKey(0)
cv2.destroyAllWindows()



### 8.4.4 좋은 매칭점 찾기
## match 함수로부터 좋은 매칭점 찾기
import cv2, numpy as np

img1 = cv2.imread('taekwonv1.jpg')
img2 = cv2.imread('figures.jpg')

gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)
gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)

detector = cv2.ORB_create()

kp1, desc1 = detector.detectAndCompute(gray1, None)
kp2, desc2 = detector.detectAndCompute(gray2, None)

matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck = True)
matches = matcher.match(desc1, desc2)

## 여기서부터 핵심코드
# x.distance가 뭘까?? matches 리스트에 담긴 각각의 Dmatch 객체는 (queryidx, trainidx, imgidx, distance) 4개의 키값을 가진다. 
# 이 중에서 유사도 거리를 가지고 있는 distance를 기준으로 한다는 것이다.
# sorted 함수는 객체들을 distance로(accending) 정렬한다. 큰 순서대로 하고싶으면 reverse = True
matches = sorted(matches, key = lambda x:x.distance) 

min_dist, max_dist = matches[0].distance, matches[-1].distance

# 최소거리(min_dist)를 기준으로 최대거리 방향으로 20% 안에 있는 값들만 쓰겠다. [최소거리==--------최대거리] '='까지의 거리
ratio = 0.2
good_thresh = (max_dist - min_dist) * ratio + min_dist

good_matches = [i for i in matches if i.distance < good_thresh]
print('matches: %d/%d, min:%.2f, max:%.2f, thresh:%.2f'%(len(good_matches), len(matches), min_dist, max_dist, good_thresh))

res = cv2.drawMatches(img1, kp1, img2, kp2, good_matches, None, flags = cv2.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS)

cv2.imshow('MFMatcher + ORB', res)
cv2.waitKey(0)
cv2.destroyAllWindows() 




## knnMatch 함수로부터 좋은 매칭점 찾기
import cv2, numpy as np

img1 = cv2.imread('taekwonv1.jpg')
img2 = cv2.imread('figures.jpg')

gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)
gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)

detector = cv2.ORB_create()

kp1, desc1 = detector.detectAndCompute(gray1, None)
kp2, desc2 = detector.detectAndCompute(gray2, None)

matcher = cv2.BFMatcher(cv2.NORM_HAMMING)
matches = matcher.knnMatch(desc1, desc2, 2)

ratio = 0.75
good_matches = [first for first, second in matches if first.distance < second.distance * ratio]
print('matches:%d/%d'%(len(good_matches), len(matches)))

res = cv2.drawMatches(img1, kp1, img2, kp2, matches, None, flags = cv2.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS)

cv2.imshow('MFMatcher + ORB', res)
cv2.waitKey(0)
cv2.destroyAllWindows() 




### 8.4.5 매칭 영역 원근 변환
# 매칭점 원근 변환으로 영역 찾기
import cv2, numpy as np

img1 = cv2.imread('taekwonv1.jpg')
img2 = cv2.imread('figures.jpg')

gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)
gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)

detector = cv2.ORB_create()

kp1, desc1 = detector.detectAndCompute(gray1, None)
kp2, desc2 = detector.detectAndCompute(gray2, None)

matcher = cv2.BFMatcher(cv2.NORM_HAMMING)
matches = matcher.knnMatch(desc1, desc2, 2)

ratio = 0.75
good_matches = [first for first, second in matches if first.distance < second.distance * ratio]
print('matches:%d/%d'%(len(good_matches), len(matches)))

'''
good_matches는 위에 코드에서 2개의 이웃하는 매칭점의 거리 중에 앞에 것의 거리가 뒤에 것의 거리의 75% 이내인 것만 모은것이다.
이것들의 index값을 가져온다는 것은 무슨 의미일까? 

우선 첫번째로
ORB-디스크립터(detector)로 구해진 gray1과 gray2의 특징점 최대수는 500개이다. 
이것들을 특징 매칭(cv2.BFMatcher, cv2.FlannBasedMatcher)을 통해 서로 다른 두 영상에서 구한 keypoints와 descriptors들을 각각 비교해서 
그 거리가 비슷한 것끼리 짝짓는다. 따라서 matches 수의 수도 desc1, desc2의 거리를 비교한 것과 같은 500개이다. (knn=2 이기 때문에 튜플형태(1,2)로 구성)
튜플구조에서 첫번째가 두번쨰의 75% 안에 들어가는 것만 담은 것이 good_matches. 

두번째로
good_matches안에 들어있는 비슷하다고 계산된 객체들은 여전히 객체로서 유효하기 때문에 queryIdx 함수를 이용해서 index값을 추출한다면 
desc1, desc2로 구해진 matches에서의 index를 반환한다. 
그리고 desc1, desc2과 keypoint1, keypoint2은 같은 좌표값들에 대한 계산들이기 때문에 같은 index를 사용하게 된다.

따라서 결론은
kp1[m.queryIdx].pt는 mathcer(desc1, desc2)을 통해 good_matchimg 된 매칭점들의 index 값은 keypoints의 index 값과 똑같으므로
결국에는 good_matching 된 keypoints의 (x, y)좌표를 반환한다.

'''
src_pts = np.float32([kp1[m.queryIdx].pt for m in good_matches])
dst_pts = np.float32([kp2[m.trainIdx].pt for m in good_matches])

# mtrx, 원근변환 행렬 출력
mtrx, mask = cv2.findHomography(src_pts, dst_pts)

h, w = img1.shape[:2]

pts = np.float32([[[0, 0]], [[0, h-1]], [[w-1, h-1]], [[w-1, 0]]])

dst = cv2.perspectiveTransform(pts, mtrx)

img2 = cv2.polylines(img2, [np.int32(dst)], True, (0,0,255), 3, cv2.LINE_AA)

res = cv2.drawMatches(img1, kp1, img2, kp2, good_matches, None, flags = cv2.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS)

cv2.imshow('Matching Homograhpy', res)
cv2.waitKey()
cv2.destroyAllWindows()



# RANSAC 원근 변환 근사 계산으로 나쁜 매칭 제거
import cv2, numpy as np

img1 = cv2.imread('taekwonv1.jpg')
img2 = cv2.imread('figures2.jpg')

gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)
gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)

detector = cv2.ORB_create()

kp1, desc1 = detector.detectAndCompute(gray1, None)
kp2, desc2 = detector.detectAndCompute(gray2, None)

matcher = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck = True)
matches = matcher.match(desc1, desc2)

matches = sorted(matches, key = lambda x:x.distance) 
res1 = cv2.drawMatches(img1, kp1, img2, kp2, matches, None, flags = cv2.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS)


src_pts = np.float32([kp1[m.queryIdx].pt for m in matches])
dst_pts = np.float32([kp2[m.trainIdx].pt for m in matches])

mtrx, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)

h, w = img1.shape[:2]
pts = np.float32([[[0, 0]], [[0, h-1]], [[w-1, h-1]], [[w-1, 0]]])

dst = cv2.perspectiveTransform(pts, mtrx)
img2 = cv2.polylines(img2, [np.int32(dst)], True, 255, 3, cv2.LINE_AA)

# 정상치만 그리기
matchesMask = mask.ravel().tolist()
res2 = cv2.drawMatches(img1, kp1, img2, kp2, matches, None, matchesMask=matchesMask, flags = cv2.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS)

accuracy = float(mask.sum()) / mask.size
print("accuracy: %d/%d(%.2f%%)"% (mask.sum(), mask.size, accuracy))


cv2.imshow('Matching-All', res1)
cv2.imshow('Matching-Inlier', res2)
cv2.waitKey()
cv2.destroyAllWindows()




# 카메라로 객체 매칭
import cv2, numpy as np

img1 = None
win_name = 'camera matching'
MIN_MATCH = 5

detector = cv2.ORB_create(1000)

FLANN_INDEX_LSH = 6
index_params = dict(algorithm = FLANN_INDEX_LSH,
                    talbe_number = 6,
                    key_size = 12,
                    multi_probe_level = 1)
search_params = dict(checks = 32)

matcher = cv2.FlannBasedMatcher(index_params, search_params)

cap = cv2.VideoCapture(0)
cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)

while cap.isOpened():
    rat, frame = cap.read()
    
    if img1 is None:
        res = frame
    else:
        img2 = frame # 카메라에 보여지는 화면은 img2, img1에는 selectROI로 특정된 부분이 담긴다.
        gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)
        gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)
        
        kp1, desc1 = detector.detectAndCompute(gray1, None)
        kp2, desc2 = detector.detectAndCompute(gray2, None)
        
        matches = matcher.knnMatch(desc1, desc2, 2)
        
        ratio = 0.75
        good_matches = [m[0] for m in matches if len(m) == 2 and m[0].distance < m[1].distance * ratio]
        print('matches:%d/%d'%(len(good_matches), len(matches)))
        
        # good_matches의 수가 MIN_MATCH 보다 작을 때는 아무것도 나타나면 안되므로 마스크를 0으로 채운다.
        matchesMask = np.zeros(len(good_matches)).tolist()
        
        if len(good_matches) > MIN_MATCH:
            src_pts = np.float32([kp1[m.queryIdx].pt for m in good_matches])
            dst_pts = np.float32([kp2[m.trainIdx].pt for m in good_matches])
            
            mtrx, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)
            accuracy = float(mask.sum()) / mask.size
            print("accuracy: %d/%d(%.2f%%)"% (mask.sum(), mask.size, accuracy))
            if mask.sum() > MIN_MATCH:
                matchesMask = mask.ravel().tolist() 
                
                h, w = img1.shape[:2]
                pts = np.float32([ [[0, 0]], [[0, h-1]], [[w-1, h-1]], [[w-1, 0]] ])
                
                # 원본영상(img1)의 좌표가 원근변환행렬(mtrx)을 통해 대상영상(img2)에서 어디에 위치해야 하는지를 
                # 출력하는 것이 cv2.perspectiveTransform의 역할
                dst = cv2.perspectiveTransform(pts, mtrx)
                img2 = cv2.polylines(img2, [np.int32(dst)], True, 255, 3, cv2.LINE_AA)
                
        res = cv2.drawMatches(img1, kp1, img2, kp2, good_matches, None, \
                              matchesMask=matchesMask, flags = cv2.DRAW_MATCHES_FLAGS_NOT_DRAW_SINGLE_POINTS)
    
    cv2.imshow('win_name', res)
   
    key = cv2.waitKey(1)
    if key == 27: # esc 키
        break
    elif key == ord(' '):
        x, y, w, h = cv2.selectROI(win_name, frame, True)
        if w and h:
            img1 = frame[y:y+h, x:x+w]

    
cap.release()
cv2.destroyAllWindows() 
