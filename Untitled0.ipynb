{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNFijSn08h4uiFFmAOvkzSF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jonghechoi/jonghe/blob/master/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vmLnFqiIH8c",
        "colab_type": "code",
        "outputId": "85b0a6af-0b95-465e-f101-820eff58ec7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqPv7TM_IQEb",
        "colab_type": "code",
        "outputId": "c304ad9f-4e91-4157-cfda-c0e1af74011c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "print(tf.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-10924bd9ad1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tensorflow_version 2.x'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lszUpoU_zDO6",
        "colab_type": "code",
        "outputId": "e29465a8-fbe2-4db6-8e57-ffa907af4020",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "print(tf.__version__)\n",
        "\n",
        "layers = tf.keras.layers\n",
        "\n",
        "class _IdentityBlock(tf.keras.Model):\n",
        "    \"\"\"_IdentityBlock is the block that has no conv layer at shortcut.\n",
        "    Args:\n",
        "      kernel_size: the kernel size of middle conv layer at main path\n",
        "      filters: list of integers, the filters of 3 conv layer at main path\n",
        "      stage: integer, current stage label, used for generating layer names\n",
        "      block: 'a','b'..., current block label, used for generating layer names\n",
        "      data_format: data_format for the input ('channels_first' or 'channels_last').\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self,\n",
        "                 kernel_size,\n",
        "                 filters,\n",
        "                 stage,\n",
        "                 block,\n",
        "                 data_format,\n",
        "                 strides=(1,1)):\n",
        "        # super가 뭐지?? 부모클래스(tf.keras.Model)를 초기화해주는 장치\n",
        "        # 인자를 주지 않으면 자동으로 현재 클래스로 설정되고, 준다면 첫번째 인자는 클래스, 두번째는 인스턴스가 들어간다.\n",
        "        # 첫번째 인자에 들어간 클래스의 부모클래스가 선택된다\n",
        "        super(_IdentityBlock, self).__init__(name='')\n",
        "        filters1, filters2, filters3 = filters\n",
        "        \n",
        "        conv_name_base = 'res' + str(stage) + block + '_branch'\n",
        "        bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
        "        # 'channels_first'라면 channels의 수가 (0,1,2,3) 중에 1번 인자에 들어가게 되므로 축을 1로 삼는다.\n",
        "        bn_axis = 1 if data_format == 'channels_first' else 3\n",
        "        \n",
        "        self.conv2a = layers.Conv2D(\n",
        "                filters1, \n",
        "                (1, 1),\n",
        "                strides=strides,\n",
        "                name=conv_name_base + '2a',\n",
        "                data_format=data_format)\n",
        "        self.bn2a = layers.BatchNormalization(axis = bn_axis, name=bn_name_base + '2a')\n",
        "       \n",
        "        self.conv2b = layers.Conv2D(\n",
        "                filters2,\n",
        "                kernel_size,\n",
        "                kernel_initializer='he_normal',\n",
        "                strides=strides,\n",
        "                padding='same',\n",
        "                data_format=data_format,\n",
        "                name=conv_name_base + '2b')\n",
        "        self.bn2b = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + '2b')\n",
        "        \n",
        "        self.conv2c = layers.Conv2D(\n",
        "                filters3,\n",
        "                (1, 1),\n",
        "                strides=strides,\n",
        "                name=conv_name_base + '2c',\n",
        "                data_format=data_format)\n",
        "        self.bn2c = layers.BatchNormalization(axis=bn_axis, name=bn_name_base + '2c')\n",
        "        \n",
        "    ''' full pre-activation 적용 '''\n",
        "    def call(self, input_tensor, training=True):\n",
        "        x = self.bn2a(input_tensor, training = training)\n",
        "        x = tf.nn.relu(x)\n",
        "        x = self.conv2a(x)\n",
        "\n",
        "        x = self.bn2b(x, training = training)\n",
        "        x = tf.nn.relu(x)\n",
        "        x = self.conv2b(x)\n",
        "\n",
        "        x = self.bn2c(x, training = training)\n",
        "        x = tf.nn.relu(x)\n",
        "        x = self.conv2c(x)\n",
        "        \n",
        "        x += input_tensor\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class _ConvBlock(tf.keras.Model):\n",
        "  \n",
        "    \"\"\"_ConvBlock은 short connection에서 feature의 개수를 맞춰주기 위해 1x1컨볼루션 연산을 수행\n",
        "    Args:\n",
        "        kernel_size: the kernel size of middle conv layer at main path\n",
        "        filters: list of integers, the filters of 3 conv layer at main path\n",
        "        stage: integer, current stage label, used for generating layer names\n",
        "        block: 'a','b'..., current block label, used for generating layer names\n",
        "        data_format: data_format for the input ('channels_first' or\n",
        "        'channels_last').\n",
        "        strides: strides for the convolution. Note that from stage 3, the first\n",
        "        conv layer at main path is with strides=(2,2), and the shortcut should\n",
        "        have strides=(2,2) as well.\n",
        "        \n",
        "        기존 _ConvBlock에서 몇가지를 수정한다.\n",
        "        1. conv_shortcut의 필터의 수를 filter1 에서 filter3로 수정한다.\n",
        "        2. padding='same'으로 하여 이미지 크기가 축소되지 않게 한다.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self,\n",
        "                 kernel_size,\n",
        "                 filters,\n",
        "                 stage,\n",
        "                 block,\n",
        "                 data_format,\n",
        "                 strides=(1,1)):\n",
        "        super(_ConvBlock, self).__init__(name='')\n",
        "        filters1, filters2, filters3 = filters\n",
        "        \n",
        "        conv_name_base = 'res' + str(stage) + block + '_branch'\n",
        "        bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
        "        bn_axis = 1 if data_format == 'channels_first' else 3\n",
        "    \n",
        "        self.conv2a = layers.Conv2D(\n",
        "                filters1,\n",
        "                (1,1),\n",
        "                strides=strides,\n",
        "                name = conv_name_base + '2a',\n",
        "                data_format = data_format)\n",
        "        self.bn2a = layers.BatchNormalization(axis = bn_axis, name = bn_name_base + '2a')\n",
        "\n",
        "        self.conv2b = layers.Conv2D(\n",
        "                filters2,\n",
        "                kernel_size,\n",
        "                kernel_initializer='he_normal',\n",
        "                strides=strides,\n",
        "                padding='same',\n",
        "                name = conv_name_base + '2b',\n",
        "                data_format = data_format)\n",
        "        self.bn2b = layers.BatchNormalization(axis = bn_axis, name = bn_name_base + '2b')\n",
        "\n",
        "        self.conv2c = layers.Conv2D(\n",
        "                filters3,\n",
        "                (1,1),\n",
        "                strides=strides,\n",
        "                name = conv_name_base + '2c',\n",
        "                data_format = data_format)\n",
        "        self.bn2c = layers.BatchNormalization(axis = bn_axis, name = bn_name_base + '2c')\n",
        "\n",
        "        # conv_shortcut의 필터의 수를 building_block의 마지막 필터 개수인 filter3로 해야하는 것 아닌가 ?\n",
        "        # 일단은 filter1 -> filter3 로 변경해봄.\n",
        "        self.conv_shortcut = layers.Conv2D(\n",
        "                filters3,\n",
        "                (1,1),\n",
        "                strides=strides,\n",
        "                name = conv_name_base + '1',\n",
        "                data_format = data_format)\n",
        "\n",
        "    def call(self, input_tensor, training=True):\n",
        "        x = self.bn2a(input_tensor, training = training)\n",
        "        x = tf.nn.relu(x)\n",
        "        x = self.conv2a(x)\n",
        "\n",
        "        x = self.bn2b(x, training = training)\n",
        "        x = tf.nn.relu(x)\n",
        "        x = self.conv2b(x)\n",
        "\n",
        "        x = self.bn2c(x, training = training)\n",
        "        x = tf.nn.relu(x)\n",
        "        x = self.conv2c(x)\n",
        "        \n",
        "        shortcut = self.conv_shortcut(input_tensor)     \n",
        "        \n",
        "        x += shortcut\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class ResNet50(tf.keras.Model):\n",
        "    \"\"\"Instantiates the ResNet50 architecture.\n",
        "    Args:\n",
        "      data_format: format for the image. Either 'channels_first' or 'channels_last'.\n",
        "      'channels_first' is typically faster on GPUs while 'channels_last' is typically faster on CPUs.\n",
        "      See \"https://www.tensorflow.org/performance/performance_guide#data_formats\"\n",
        "      \n",
        "      name: Prefix applied to names of variables created in the model.\n",
        "      \n",
        "      trainable: Is the model trainable? If true, performs backward and optimization after call() method.\n",
        "      \n",
        "      include_top: whether to include the fully-connected layer at the top of the network.\n",
        "      \n",
        "      pooling: Optional pooling mode for feature extraction when `include_top` is `False`.\n",
        "        - `None` means that the output of the model will be the 4D tensor output of the last convolutional layer.\n",
        "        - `avg` means that global average pooling will be applied to the output of the last convolutional layer,\n",
        "           and thus the output of the model will be a 2D tensor.\n",
        "        - `max` means that global max pooling will be applied.\n",
        "        \n",
        "      classes: optional number of classes to classify images into, only to be specified if `include_top` is True.\n",
        "    \n",
        "    Raises:\n",
        "        ValueError: in case of invalid argument for data_format.\n",
        "    \"\"\"\n",
        "    \n",
        "    \n",
        "    def __init__(self,\n",
        "                 data_format,\n",
        "                 name = '',\n",
        "                 trainable = True,\n",
        "                 include_top = True,\n",
        "                 pooling = None,\n",
        "                 classes = 10):\n",
        "        super(ResNet50, self).__init__(name = name)\n",
        "        \n",
        "        valid_channel_values = ('channels_first', 'channels_last')\n",
        "        if data_format not in valid_channel_values:\n",
        "            raise ValueError('Unknown data_format: %s. Valid values: %s' % (data_format, valid_channel_values))\n",
        "        self.include_top = include_top\n",
        "        \n",
        "        def conv_block(filters, stage, block, strides=(1,1)):\n",
        "            return _ConvBlock((3,3), filters, stage = stage, block = block, data_format = data_format, strides = strides)\n",
        "        \n",
        "        def id_block(filters, stage, block):\n",
        "            return _IdentityBlock((3,3), filters, stage = stage, block = block, data_format = data_format)\n",
        "        \n",
        "        self.conv1 = layers.Conv2D(\n",
        "                64,\n",
        "                (3,3),\n",
        "                strides = (1,1),\n",
        "                padding = 'same',\n",
        "                data_format = data_format,\n",
        "                name = 'conv1')\n",
        "        \n",
        "        bn_axis = 1 if data_format == 'channels_first' else 3\n",
        "        self.bn_conv1 = layers.BatchNormalization(axis = bn_axis, name = 'bn_conv1')\n",
        "        self.max_pool = layers.MaxPooling2D(\n",
        "                (3,3), strides = (2,2), data_format = data_format)\n",
        "        \n",
        "        self.l2a = conv_block([64, 64, 256], stage = 2, block = 'a', strides=(1, 1))\n",
        "        self.l2b = id_block([64,64,256], stage = 2, block = 'b')\n",
        "        self.l2c = id_block([64,64,256], stage = 2, block = 'c')\n",
        "        \n",
        "        self.l3a = conv_block([128,128,512], stage = 3, block = 'a')\n",
        "        self.l3b = id_block([128,128,512], stage = 3, block = 'b')\n",
        "        self.l3c = id_block([128,128,512], stage = 3, block = 'c')\n",
        "        self.l3d = id_block([128,128,512], stage = 3, block = 'd')\n",
        "        \n",
        "        self.l4a = conv_block([256,256,1024], stage = 4, block = 'a')\n",
        "        self.l4b = id_block([256,256,1024], stage = 4, block = 'b')\n",
        "        self.l4c = id_block([256,256,1024], stage = 4, block = 'c')\n",
        "        self.l4d = id_block([256,256,1024], stage = 4, block = 'd')\n",
        "        self.l4e = id_block([256,256,1024], stage = 4, block = 'e')\n",
        "        self.l4f = id_block([256,256,1024], stage = 4, block = 'f')\n",
        "        \n",
        "        self.l5a = conv_block([512,512,2048], stage = 5, block = 'a')\n",
        "        self.l5b = id_block([512,512,2048], stage = 5, block = 'b')\n",
        "        self.l5c = id_block([512,512,2048], stage = 5, block = 'c')\n",
        "    \n",
        "        self.avg_pool = layers.AveragePooling2D((7,7), strides = (2,2), data_format = data_format)\n",
        "        \n",
        "        if self.include_top:\n",
        "            self.flatten = layers.Flatten()\n",
        "            self.fc100 = layers.Dense(classes, activation='softmax', name = 'fc100')\n",
        "        else:\n",
        "            reduction_indices = [1, 2] if data_format == 'channels_last' else [2, 3]\n",
        "            reduction_indices = tf.constant(reduction_indices)\n",
        "            if pooling == 'avg':\n",
        "                self.global_pooling = functools.partial(\n",
        "                        tf.reduce_mean,\n",
        "                        reduction_indices = reduction_indices,\n",
        "                        keep_dims = False)\n",
        "            elif pooling == 'max':\n",
        "                self.global_pooling = functools.partial(\n",
        "                        tf.reduce_max, reduction_indices = reduction_indices, keep_dims = False)\n",
        "            else:\n",
        "                self.global_pooling = None\n",
        "    \n",
        "    \n",
        "    def call(self, inputs, training=True):\n",
        "        x = self.conv1(inputs)\n",
        "        \n",
        "        x = self.bn_conv1(x, training=training)\n",
        "        x = tf.nn.relu(x)\n",
        "        x = self.max_pool(x)\n",
        "\n",
        "        \n",
        "        x = self.l2a(x, training=training)\n",
        "        x = self.l2b(x, training=training)\n",
        "        x = self.l2c(x, training=training)\n",
        "\n",
        "\n",
        "        x = self.l3a(x, training=training)\n",
        "        x = self.l3b(x, training=training)\n",
        "        x = self.l3c(x, training=training)\n",
        "        x = self.l3d(x, training=training)\n",
        "\n",
        "        \n",
        "        x = self.l4a(x, training=training)\n",
        "        x = self.l4b(x, training=training)\n",
        "        x = self.l4c(x, training=training)\n",
        "        x = self.l4d(x, training=training)\n",
        "        x = self.l4e(x, training=training)\n",
        "        x = self.l4f(x, training=training)\n",
        "\n",
        "\n",
        "        x = self.l5a(x, training=training)\n",
        "        x = self.l5b(x, training=training)\n",
        "        x = self.l5c(x, training=training)\n",
        "        \n",
        "        x = self.avg_pool(x)\n",
        "        \n",
        "        if self.include_top:\n",
        "            x = self.flatten(x)\n",
        "            return self.fc100(x)\n",
        "        elif self.global_pooling:\n",
        "          return self.global_pooling(x)\n",
        "        else:\n",
        "          return x"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIGmr_MPHKHS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qB1micJuzKVU",
        "colab_type": "code",
        "outputId": "8a555dc4-7e3b-42d3-dd62-f5ab8bcedd82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import functools\n",
        "from keras.utils import to_categorical\n",
        "from tensorflow.keras.datasets import cifar100\n",
        "\n",
        "def normalize(X_train, X_test):\n",
        "    mean = np.mean(X_train, axis=(0, 1, 2, 3))\n",
        "    std = np.std(X_train, axis=(0, 1, 2, 3))\n",
        "\n",
        "    X_train = (X_train - mean) / std\n",
        "    X_test = (X_test - mean) / std \n",
        "\n",
        "    return X_train, X_test\n",
        "  \n",
        "def load_cifar100():\n",
        "    (train_data, train_labels), (test_data, test_labels) = cifar100.load_data()\n",
        "    # train_data = train_data / 255.0\n",
        "    # test_data = test_data / 255.0\n",
        "    train_data, test_data = normalize(train_data, test_data)\n",
        "\n",
        "    train_labels = to_categorical(train_labels, 100)\n",
        "    test_labels = to_categorical(test_labels, 100)\n",
        "\n",
        "    seed = 777\n",
        "    np.random.seed(seed)\n",
        "    np.random.shuffle(train_data)\n",
        "    np.random.seed(seed)\n",
        "    np.random.shuffle(train_labels)\n",
        "\n",
        "    return train_data, train_labels, test_data, test_labels\n",
        "\n",
        "# dataset 만들기\n",
        "train_data, train_labels, test_data, test_labels = load_cifar100()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "169009152/169001437 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTVkh6IyNzG_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "batch_size = 128\n",
        "dataset = tf.data.Dataset.from_tensor_slices((train_data, train_labels)).repeat()\n",
        "dataset = dataset.shuffle(buffer_size=200).batch(batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_3SQpgZHSIz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def data_format():\n",
        "    return 'channels_last' if tf.test.is_gpu_available() else 'channels_first'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bQIQKTGzXj5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss = tf.keras.losses.CategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "\n",
        "# 정확도에 대한 metric의 인스턴스 생성\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.CategoricalAccuracy(name='train_accuracy')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YnCtEyizd9Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "myResNet50 = ResNet50(data_format())\n",
        "    \n",
        "\n",
        "# tape에 학습과정을 기록\n",
        "#@tf.function\n",
        "def train_step(x, y):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = myResNet50(x, training=True)     \n",
        "        loss_value = loss(y, predictions)\n",
        "    \n",
        "    # 모델에서 기록된 순전파의 내용들을 tape에서 거꾸로 되감기하면서 loss값에 대해서 가중치의 미분값을 계산\n",
        "    gradients = tape.gradient(loss_value, myResNet50.trainable_variables)\n",
        "    # 계산된 미분값을 가중치에 적용\n",
        "    optimizer.apply_gradients(zip(gradients, myResNet50.trainable_variables))\n",
        "        \n",
        "    # 현재까지 수행된 전체에 대한 모델의 정확도를 갱신\n",
        "    train_loss(loss_value)\n",
        "    train_accuracy(y, predictions)\n",
        "\n",
        "    \n",
        "\n",
        "    \n",
        "    \n",
        "for epoch in range(5):\n",
        "    for step, (x, y) in enumerate(dataset, 1):\n",
        "        train_step(x, y)  \n",
        "        \n",
        "        template = 'step: {}, train_loss: {}, train_accuracy: {}'\n",
        "        print( template.format(step,\n",
        "                               train_loss.result(),\n",
        "                               train_accuracy.result()*100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBLh7mdR-d97",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "myResNet50 = ResNet50(data_format())\n",
        "\n",
        "myResNet50.compile(optimizer=tf.keras.optimizers.Adam(0.001),\n",
        "                   loss= tf.keras.losses.CategoricalCrossentropy(),\n",
        "                   metrics=[tf.keras.metrics.CategoricalAccuracy()])\n",
        "\n",
        "\n",
        "myResNet50.fit(train_data, train_labels, batch_size=128, epochs=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSvymMReN3HE",
        "colab_type": "code",
        "outputId": "32ebef11-0e3c-4007-acb4-c0701fbd8a53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 76
        }
      },
      "source": [
        "tf.keras.utils.plot_model(myResNet50, show_shapes=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAH8AAAA8CAYAAABPcWXRAAAABmJLR0QA/wD/AP+gvaeTAAAGqElE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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnkweWjhHVmi",
        "colab_type": "text"
      },
      "source": [
        "# cifar10으로 100에폭 학습시켰을 때의 시간\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77UvL892Hk8s",
        "colab_type": "code",
        "outputId": "6eb49533-40d7-4053-95d4-a4aafb53474d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import functools\n",
        "from keras.utils import to_categorical\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "\n",
        "def normalize(X_train, X_test):\n",
        "    mean = np.mean(X_train, axis=(0, 1, 2, 3))\n",
        "    std = np.std(X_train, axis=(0, 1, 2, 3))\n",
        "\n",
        "    X_train = (X_train - mean) / std\n",
        "    X_test = (X_test - mean) / std \n",
        "\n",
        "    return X_train, X_test\n",
        "  \n",
        "def load_cifar10():\n",
        "    (train_data, train_labels), (test_data, test_labels) = cifar10.load_data()\n",
        "    # train_data = train_data / 255.0\n",
        "    # test_data = test_data / 255.0\n",
        "    train_data, test_data = normalize(train_data, test_data)\n",
        "\n",
        "    train_labels = to_categorical(train_labels, 10)\n",
        "    test_labels = to_categorical(test_labels, 10)\n",
        "\n",
        "    seed = 777\n",
        "    np.random.seed(seed)\n",
        "    np.random.shuffle(train_data)\n",
        "    np.random.seed(seed)\n",
        "    np.random.shuffle(train_labels)\n",
        "\n",
        "    return train_data, train_labels, test_data, test_labels\n",
        "\n",
        "# dataset 만들기\n",
        "train_data, train_labels, test_data, test_labels = load_cifar10()\n",
        "\n",
        "batch_size = 128\n",
        "dataset_cifar10 = tf.data.Dataset.from_tensor_slices((train_data, train_labels)).repeat()\n",
        "dataset_cifar10 = dataset_cifar10.shuffle(buffer_size=200).batch(batch_size)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPmq_S6TH-fK",
        "colab_type": "code",
        "outputId": "f89bfdc2-e1cd-4555-f49a-26f7d27b2bc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(train_data.shape)\n",
        "print(train_labels.shape)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50000, 32, 32, 3)\n",
            "(50000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6g2Ndz5H1au",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "\n",
        "myResNet50 = ResNet50(data_format())\n",
        "\n",
        "myResNet50.compile(optimizer=tf.keras.optimizers.Adam(),\n",
        "                   loss= tf.keras.losses.CategoricalCrossentropy(),\n",
        "                   metrics=[tf.keras.metrics.CategoricalAccuracy()])\n",
        "\n",
        "\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
        "\n",
        "time_start = time.time()\n",
        "\n",
        "history = myResNet50.fit(train_data, train_labels, batch_size=128, epochs=50, callbacks=[callback], \n",
        "                         validation_data=(test_data, test_labels))\n",
        "\n",
        "time_end = time.time() - time_start\n",
        "\n",
        "print( time.strftime(\"%H 시간: %M 분: %S 초\", time.gmtime(time_end)) )\n",
        "\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "myResNet50.save_weights('my_resnet50_cifar10.h5')\n",
        "\n",
        "acc = history.history['categorical_accuracy']\n",
        "val_acc = history.history['val_categorical_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "accuracy_dict = {}\n",
        "for i, j in zip(['acc', 'val_acc', 'loss', 'val_loss'], [acc, val_acc, loss, val_loss]):\n",
        "    accuracy_dict[i] = j\n",
        "\n",
        "f = open('my_resnet50_cifar10_info.pickle', 'wb')\n",
        "pickle.dump(accuracy_dict, f)\n",
        "f.close()\n",
        "\n",
        "# pickle 파일로부터 결과값 불러오기\n",
        "ff = open('my_resnet50_cifar10_info.pickle', 'rb')\n",
        "accuracy = pickle.load(ff)\n",
        "ff.close()\n",
        "\n",
        "accuracy_redict = {}\n",
        "for idx, i in accuracy.items():\n",
        "    accuracy_redict[idx] = i\n",
        "\n",
        "epochs = range(len(accuracy_redict['acc']))\n",
        "\n",
        "plt.plot(epochs, accuracy_redict['acc'], 'b', label='Accuracy', linewidth=1)\n",
        "plt.plot(epochs, accuracy_redict['val_acc'], 'r', label='Val_accuracy', linewidth=1)\n",
        "plt.title('Train accuracy & Validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "# validation 데이터의 loss가 낮아지다가 높아지기시작하면 이는 과적합의 신호\n",
        "plt.plot(epochs, accuracy_redict['loss'], 'b', label='Loss', linewidth=1)\n",
        "plt.plot(epochs, accuracy_redict['val_loss'], 'r', label='Val_loss', linewidth=1)\n",
        "plt.title('Train loss & Validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}